{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "89541229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.interpolate import interp1d\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5138be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_root: str = \"\"\n",
    "    out_root: str = \"features_out_data_feature_selection\"\n",
    "    splits: Tuple[int, int] = (1, 20)\n",
    "    valid_filters: Tuple[str, ...] = (\"u\", \"g\", \"r\", \"i\", \"z\", \"y\")\n",
    "\n",
    "    # Cleaning\n",
    "    dropna_cols: Tuple[str, ...] = (\"object_id\", \"Time (MJD)\", \"Flux\", \"Flux_err\", \"Filter\")\n",
    "    min_fluxerr: float = 0.0\n",
    "    merge_duplicates: bool = True\n",
    "\n",
    "    # Time features\n",
    "    snr_det_threshold: float = 3.0\n",
    "    snr_strong_threshold: float = 5.0\n",
    "    \n",
    "    # Color features\n",
    "    color_max_dt_days: float = 1.0\n",
    "\n",
    "    # Shape fitting (NOW ENABLED!)\n",
    "    do_shape_fitting: bool = True  \n",
    "    shape_fit_min_points: int = 6  \n",
    "    shape_fit_min_peak_snr: float = 5.0  \n",
    "    shape_fit_per_filter: bool = True  \n",
    "\n",
    "\n",
    "    # Missing handling\n",
    "    do_imputation: bool = True\n",
    "\n",
    "    # Feature selection\n",
    "    do_feature_selection: bool = True  \n",
    "    corr_threshold: float = 0.95\n",
    "    rf_top_k: int = 150\n",
    "    random_state: int = 42\n",
    "    \n",
    "    # Performance\n",
    "    use_multiprocessing: bool = False \n",
    "    n_jobs: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# COSMOLOGY HELPERS - OPTIMIZED\n",
    "# =========================\n",
    "\n",
    "C_KM_S = 299792.458\n",
    "\n",
    "_DL_CACHE = {}\n",
    "\n",
    "def E_z(z: np.ndarray, Om0: float = 0.3) -> np.ndarray:\n",
    "    \"\"\"E(z)=H(z)/H0 for flat LCDM.\"\"\"\n",
    "    Ol0 = 1.0 - Om0\n",
    "    return np.sqrt(Om0 * (1 + z)**3 + Ol0)\n",
    "\n",
    "def luminosity_distance_Mpc_fast(z: np.ndarray, H0: float = 70.0, Om0: float = 0.3) -> np.ndarray:\n",
    "    z = np.asarray(z, dtype=float)\n",
    "    out = np.full_like(z, np.nan, dtype=float)\n",
    "    \n",
    "    mask = np.isfinite(z) & (z >= 0)\n",
    "    if not mask.any():\n",
    "        return out\n",
    "    \n",
    "    z_valid = z[mask]\n",
    "    z_unique = np.unique(z_valid)\n",
    "    \n",
    "    # Compute for unique z values only\n",
    "    for zz in z_unique:\n",
    "        cache_key = f\"{zz:.6f}_{H0}_{Om0}\"\n",
    "        \n",
    "        if cache_key not in _DL_CACHE:\n",
    "            if zz == 0:\n",
    "                _DL_CACHE[cache_key] = 0.0\n",
    "            else:\n",
    "                grid = np.linspace(0.0, zz, 256)  # Reduced from 512\n",
    "                integral = np.trapz(1.0 / E_z(grid, Om0=Om0), grid)\n",
    "                d_c = (C_KM_S / H0) * integral\n",
    "                d_l = (1.0 + zz) * d_c\n",
    "                _DL_CACHE[cache_key] = d_l\n",
    "        \n",
    "        # Fill all matching z values\n",
    "        z_mask = (z_valid == zz)\n",
    "        out[np.where(mask)[0][z_mask]] = _DL_CACHE[cache_key]\n",
    "    \n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "496eca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_mag_from_flux_uJy(flux_uJy: np.ndarray, d_L_Mpc: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert flux to absolute magnitude.\"\"\"\n",
    "    flux = np.asarray(flux_uJy, dtype=float)\n",
    "    dL = np.asarray(d_L_Mpc, dtype=float)\n",
    "    out = np.full_like(flux, np.nan, dtype=float)\n",
    "\n",
    "    mask = np.isfinite(flux) & (flux > 0) & np.isfinite(dL) & (dL > 0)\n",
    "    if not mask.any():\n",
    "        return out\n",
    "\n",
    "    m_ab = 23.9 - 2.5 * np.log10(flux[mask])\n",
    "    DM = 5.0 * np.log10(dL[mask]) + 25.0\n",
    "    out[mask] = m_ab - DM\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# STATISTICAL FEATURES \n",
    "# =========================\n",
    "\n",
    "def stats_features_per_filter_enhanced(df: pd.DataFrame, value_col: str, cfg: Config) -> pd.DataFrame:\n",
    "\n",
    "    g = df.groupby([\"object_id\", \"Filter\"])[value_col]\n",
    "\n",
    "    def mad(x):\n",
    "        \"\"\"Median Absolute Deviation\"\"\"\n",
    "        return np.median(np.abs(x - np.median(x)))\n",
    "    \n",
    "    def beyond_1std(x):\n",
    "        \"\"\"Fraction of points beyond 1 sigma\"\"\"\n",
    "        if len(x) < 3:\n",
    "            return 0.0\n",
    "        m = np.mean(x)\n",
    "        s = np.std(x)\n",
    "        return float(np.mean(np.abs(x - m) > s))\n",
    "\n",
    "    feats = g.agg(\n",
    "        mean=\"mean\",\n",
    "        std=\"std\",\n",
    "        median=\"median\",\n",
    "        min=\"min\",\n",
    "        max=\"max\",\n",
    "        q10=lambda x: np.nanpercentile(x, 10),\n",
    "        q90=lambda x: np.nanpercentile(x, 90),\n",
    "        skew=lambda x: stats.skew(x, nan_policy='omit'),\n",
    "        kurt=lambda x: stats.kurtosis(x, nan_policy='omit'),\n",
    "        mad=mad,\n",
    "        amplitude=lambda x: np.nanmax(x) - np.nanmin(x),\n",
    "        beyond_1std=beyond_1std\n",
    "    ).reset_index()\n",
    "\n",
    "    # Pivot to wide\n",
    "    wide = feats.pivot(index=\"object_id\", columns=\"Filter\")\n",
    "    wide.columns = [f\"{f}_{stat}_{value_col}\" for stat, f in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "    \n",
    "    return wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "89ec4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_features_per_filter(df: pd.DataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"Count observations per filter.\"\"\"\n",
    "    g = df.groupby([\"object_id\", \"Filter\"]).size().reset_index(name=\"n_obs\")\n",
    "    wide = g.pivot(index=\"object_id\", columns=\"Filter\", values=\"n_obs\")\n",
    "    wide.columns = [f\"n_obs_{f}\" for f in wide.columns]\n",
    "    wide = wide.fillna(0).reset_index()\n",
    "    return wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89583c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_features(df: pd.DataFrame, flux_col: str, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"SNR-based features.\"\"\"\n",
    "    out = df.copy()\n",
    "    err_col = \"Flux_err\"\n",
    "    out[\"SNR\"] = out[flux_col].values / out[err_col].values\n",
    "\n",
    "    grp = out.groupby(\"object_id\")\n",
    "    feats = pd.DataFrame({\n",
    "        \"snr_max\": grp[\"SNR\"].max(),\n",
    "        \"snr_mean\": grp[\"SNR\"].mean(),\n",
    "        \"snr_median\": grp[\"SNR\"].median(),  \n",
    "        \"snr_std\": grp[\"SNR\"].std(),\n",
    "        \"n_snr_gt3\": grp.apply(lambda x: int(np.sum(x[\"SNR\"].values > cfg.snr_det_threshold))),\n",
    "        \"n_snr_gt5\": grp.apply(lambda x: int(np.sum(x[\"SNR\"].values > cfg.snr_strong_threshold))),\n",
    "        \"frac_snr_gt3\": grp.apply(lambda x: float(np.mean(x[\"SNR\"].values > cfg.snr_det_threshold))),\n",
    "        \"frac_snr_gt5\": grp.apply(lambda x: float(np.mean(x[\"SNR\"].values > cfg.snr_strong_threshold))),\n",
    "    }).reset_index()\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_time_features_enhanced(df: pd.DataFrame, flux_col: str, cfg: Config) -> pd.DataFrame:\n",
    "    # 1. Chuẩn bị dữ liệu\n",
    "    err_col = \"Flux_err\"\n",
    "    tmp = df.copy()\n",
    "    tmp[\"SNR\"] = tmp[flux_col] / (tmp[err_col] + 1e-6)\n",
    "    \n",
    "    # Sort trước để tính toán chính xác\n",
    "    tmp = tmp.sort_values([\"object_id\", \"Time (MJD)\"])\n",
    "    grp = tmp.groupby(\"object_id\")\n",
    "\n",
    "    # =========================================================\n",
    "    # PHẦN 1: VECTORIZED AGGREGATIONS\n",
    "    # =========================================================\n",
    "    \n",
    "    aggs = grp[\"Time (MJD)\"].agg(['min', 'max'])\n",
    "    aggs.columns = ['t_min', 't_max']\n",
    "    \n",
    "    # Tính duration thủ công (An toàn hơn dùng ptp)\n",
    "    aggs['time_span_days'] = aggs['t_max'] - aggs['t_min']\n",
    "    \n",
    "    # 1.2 Peak Time & Peak Flux\n",
    "    idx_max = grp[flux_col].idxmax()\n",
    "    peak_info = tmp.loc[idx_max, [\"object_id\", \"Time (MJD)\", flux_col, \"SNR\"]].set_index(\"object_id\")\n",
    "    peak_info.columns = [\"t_peak_mjd\", \"peak_flux\", \"peak_snr\"]\n",
    "    \n",
    "    # Merge lại\n",
    "    feats = aggs.join(peak_info)\n",
    "    \n",
    "    # 1.3 Time Window Features\n",
    "    feats[\"pre_peak_duration\"] = feats[\"t_peak_mjd\"] - feats[\"t_min\"]\n",
    "    feats[\"post_peak_duration\"] = feats[\"t_max\"] - feats[\"t_peak_mjd\"]\n",
    "    feats[\"rise_fall_ratio\"] = feats[\"pre_peak_duration\"] / (feats[\"post_peak_duration\"] + 1e-5)\n",
    "\n",
    "    # =========================================================\n",
    "    # PHẦN 2: COMPLEX FEATURES \n",
    "    # =========================================================\n",
    "\n",
    "    def calc_complex_features(g):\n",
    "        t = g[\"Time (MJD)\"].values\n",
    "        f = g[flux_col].values\n",
    "        snr = g[\"SNR\"].values\n",
    "        \n",
    "        res = {}\n",
    "        \n",
    "        # --- A. Cadence ---\n",
    "        if len(t) >= 2:\n",
    "            dt = np.diff(t)\n",
    "            res['cadence_median'] = np.median(dt)\n",
    "            res['cadence_max'] = np.max(dt)\n",
    "        else:\n",
    "            res['cadence_median'] = np.nan\n",
    "            res['cadence_max'] = np.nan\n",
    "            \n",
    "        # --- B. Peak Counting ---\n",
    "        n_peaks = 0\n",
    "        if len(t) >= 5:\n",
    "            try:\n",
    "                h = f.mean() + f.std()\n",
    "                p = f.std() * 0.5\n",
    "                peaks, _ = find_peaks(f, height=h, prominence=p)\n",
    "                n_peaks = len(peaks)\n",
    "            except:\n",
    "                n_peaks = 0\n",
    "        res['n_peaks'] = n_peaks\n",
    "\n",
    "        # --- C. Robust Rise/Decay Rate ---\n",
    "        det_mask = snr > 3.0 \n",
    "        \n",
    "        if det_mask.sum() >= 4:\n",
    "            t_det = t[det_mask]\n",
    "            f_det = f[det_mask]\n",
    "            # Lấy index đỉnh tương đối trong tập detection\n",
    "            idx_peak_det = np.argmax(f_det)\n",
    "            t_peak = t_det[idx_peak_det]\n",
    "            \n",
    "            mask_rise = t_det <= t_peak\n",
    "            mask_decay = t_det >= t_peak\n",
    "            \n",
    "            def robust_slope(tx, fx):\n",
    "                if len(tx) < 2: return np.nan\n",
    "                return (fx[-1] - fx[0]) / (tx[-1] - tx[0] + 1e-5)\n",
    "\n",
    "            res['rise_rate'] = robust_slope(t_det[mask_rise], f_det[mask_rise])\n",
    "            res['decay_rate'] = robust_slope(t_det[mask_decay], f_det[mask_decay])\n",
    "        else:\n",
    "            res['rise_rate'] = np.nan\n",
    "            res['decay_rate'] = np.nan\n",
    "\n",
    "        return pd.Series(res)\n",
    "\n",
    "    complex_feats = grp.apply(calc_complex_features)\n",
    "    \n",
    "    # =========================================================\n",
    "    # PHẦN 3: KẾT HỢP\n",
    "    # =========================================================\n",
    "    final_feats = feats.join(complex_feats)\n",
    "    \n",
    "    return final_feats.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4dab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rise_decay_features_powerlaw(df: pd.DataFrame, flux_col: str, cfg: Config) -> pd.DataFrame:\n",
    "    err_col = \"Flux_err\"\n",
    "    tmp = df.copy()\n",
    "    \n",
    "    # 1. Pre-calculate SNR & Peak info (Vectorized - Fast)\n",
    "    tmp[\"SNR\"] = tmp[flux_col] / (tmp[err_col] + 1e-6)\n",
    "\n",
    "    # Tìm index của đỉnh sáng nhất cho mỗi object\n",
    "    peak_idx = tmp.groupby(\"object_id\")[flux_col].idxmax()\n",
    "    \n",
    "    # Tạo bảng lookup đỉnh\n",
    "    peak_info = tmp.loc[peak_idx, [\"object_id\", \"Time (MJD)\", \"SNR\"]].set_index(\"object_id\")\n",
    "    peak_info.columns = [\"t_peak\", \"peak_snr\"] # Đổi tên cho gọn\n",
    "\n",
    "    # Merge đỉnh vào bảng chính\n",
    "    tmp = tmp.merge(peak_info, on=\"object_id\", how=\"left\")\n",
    "    \n",
    "    # Tính thời gian tương đối\n",
    "    tmp[\"t_rel\"] = tmp[\"Time (MJD)\"] - tmp[\"t_peak\"]\n",
    "\n",
    "    # Hàm tính R2 thủ công (an toàn với NaN)\n",
    "    def compute_r2(y, yhat):\n",
    "        ss_res = np.sum((y - yhat) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        if ss_tot < 1e-9: return 0.0 # Tránh chia 0 nếu đường thẳng nằm ngang\n",
    "        return 1.0 - ss_res / ss_tot\n",
    "\n",
    "    def per_obj(g: pd.DataFrame) -> pd.Series:\n",
    "        # Lấy thông tin đỉnh từ dòng đầu tiên \n",
    "        f_peak = g[flux_col].max()\n",
    "        peak_snr = g[\"peak_snr\"].iloc[0]\n",
    "\n",
    "        # --- 1. RISE PHASE ---\n",
    "        rise_time = np.nan\n",
    "        # Lấy dữ liệu trước đỉnh\n",
    "        before = g[(g[\"t_rel\"] < 0) & (g[flux_col] > 0)].sort_values(\"t_rel\")\n",
    "\n",
    "        if len(before) >= 7 and f_peak > 0: # MIN_RISE_POINTS = 7\n",
    "            f = before[flux_col].values\n",
    "            t = before[\"t_rel\"].values\n",
    "\n",
    "            # Tìm mốc 10% và 90% độ sáng đỉnh\n",
    "            f10, f90 = 0.1 * f_peak, 0.9 * f_peak\n",
    "            \n",
    "            # Tìm index gần nhất\n",
    "            i10 = np.argmin(np.abs(f - f10))\n",
    "            i90 = np.argmin(np.abs(f - f90))\n",
    "\n",
    "            if t[i10] < t[i90]:\n",
    "                rise_time = abs(t[i90] - t[i10])\n",
    "\n",
    "        # --- 2. DECAY PHASE ---\n",
    "        pl_slope = pl_r2 = exp_r2 = decay_discrimination = np.nan\n",
    "        decay_reliable = 0\n",
    "        \n",
    "        # Lấy dữ liệu sau đỉnh (Decay)\n",
    "        # Cửa sổ 400 ngày là hợp lý cho TDE\n",
    "        after = g[\n",
    "            (g[\"t_rel\"] > 5) &           # Bỏ qua 5 ngày đầu quanh đỉnh (thường nhiễu)\n",
    "            (g[\"t_rel\"] < 400) & \n",
    "            (g[flux_col] > 0) & \n",
    "            (g[\"SNR\"] > 3)               # Chỉ fit điểm tin cậy\n",
    "        ].sort_values(\"t_rel\")\n",
    "\n",
    "        if len(after) >= 5: # MIN_DECAY_POINTS = 5\n",
    "            t_fit = after[\"t_rel\"].values\n",
    "            f_fit = after[flux_col].values\n",
    "            e_fit = after[err_col].values\n",
    "\n",
    "            # Trọng số cho fit (Inverse Variance)\n",
    "            weights = (f_fit / (e_fit + 1e-6)) ** 2\n",
    "            log_f = np.log(f_fit)\n",
    "\n",
    "            try:\n",
    "                # A. Power-law Fit (Linear on log-log)\n",
    "                # log(F) = a + b * log(t)\n",
    "                log_t = np.log(t_fit)\n",
    "                \n",
    "                # Polyfit bậc 1\n",
    "                b_pl, a_pl = np.polyfit(log_t, log_f, 1, w=np.sqrt(weights))\n",
    "                \n",
    "                # Tính R2\n",
    "                log_f_hat_pl = a_pl + b_pl * log_t\n",
    "                pl_r2 = compute_r2(log_f, log_f_hat_pl)\n",
    "                pl_slope = b_pl\n",
    "\n",
    "                # B. Exponential Fit (Linear on semi-log)\n",
    "                # log(F) = a + b * t\n",
    "                b_exp, a_exp = np.polyfit(t_fit, log_f, 1, w=np.sqrt(weights))\n",
    "                \n",
    "                # Tính R2\n",
    "                log_f_hat_exp = a_exp + b_exp * t_fit\n",
    "                exp_r2 = compute_r2(log_f, log_f_hat_exp)\n",
    "\n",
    "                # C. Discrimination Ratio\n",
    "                if exp_r2 > 0.01: # Tránh chia số quá nhỏ\n",
    "                    decay_discrimination = pl_r2 / exp_r2\n",
    "                else:\n",
    "                    decay_discrimination = 999.0 # Power law thắng tuyệt đối\n",
    "\n",
    "                # D. Reliability Check\n",
    "                # Đáng tin nếu Peak SNR cao VÀ fit tốt\n",
    "                decay_reliable = int(\n",
    "                    peak_snr > 10 and\n",
    "                    (pl_r2 > 0.6 or exp_r2 > 0.6)\n",
    "                )\n",
    "\n",
    "            except (np.linalg.LinAlgError, ValueError, TypeError):\n",
    "                pass\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return pd.Series({\n",
    "            \"rise_time_10_90\": rise_time,\n",
    "            \"pl_slope\": pl_slope,\n",
    "            \"pl_r2\": pl_r2,\n",
    "            \"exp_r2\": exp_r2,\n",
    "            \"decay_discrimination\": decay_discrimination,\n",
    "            \"decay_reliable\": decay_reliable,\n",
    "            # TDE chuẩn lý thuyết: slope ~ -5/3 (-1.67)\n",
    "            \"pl_slope_is_tde\": int(-2.2 < pl_slope < -1.2) if np.isfinite(pl_slope) else 0\n",
    "        })\n",
    "\n",
    "    feats = tmp.groupby(\"object_id\").apply(per_obj).reset_index()\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_features_unified(df: pd.DataFrame, flux_col: str, cfg: Config) -> pd.DataFrame:\n",
    "    tmp = df.copy()\n",
    "    \n",
    "    err_col = \"Flux_err\"\n",
    "    if \"SNR\" not in tmp.columns:\n",
    "        tmp[\"SNR\"] = tmp[flux_col] / (tmp[err_col] + 1e-6)\n",
    "    \n",
    "    # Lọc dữ liệu: SNR > 3 và Flux > 0\n",
    "    valid_mask = (tmp[\"SNR\"] > 3) & (tmp[flux_col] > 0)\n",
    "    tmp = tmp[valid_mask].copy()\n",
    "\n",
    "    # Convert Flux -> AB Magnitude\n",
    "    tmp[\"mag\"] = -2.5 * np.log10(tmp[flux_col]) + 23.9\n",
    "    \n",
    "    # 2. Định nghĩa các cặp màu\n",
    "    # Kết hợp cả cặp liền kề (cho SN) và cặp xa (u-z cho TDE)\n",
    "    color_pairs = [\n",
    "        ('u', 'g'), ('g', 'r'), ('r', 'i'),  # Standard\n",
    "        ('i', 'z'), ('z', 'y'),              # Red bands\n",
    "        ('u', 'z')                           # TDE specific (UV minus IR)\n",
    "    ]\n",
    "    \n",
    "    # DataFrame kết quả chứa object_id\n",
    "    unique_ids = df[\"object_id\"].unique()\n",
    "    features = pd.DataFrame({\"object_id\": unique_ids})\n",
    "    \n",
    "    # 3. Loop qua từng cặp màu\n",
    "    for f1, f2 in color_pairs:\n",
    "        # Tách data theo filter\n",
    "        cols = [\"object_id\", \"Time (MJD)\", \"mag\"]\n",
    "        df1 = tmp[tmp[\"Filter\"] == f1][cols].rename(columns={\"mag\": \"mag1\", \"Time (MJD)\": \"time\"})\n",
    "        df2 = tmp[tmp[\"Filter\"] == f2][cols].rename(columns={\"mag\": \"mag2\", \"Time (MJD)\": \"time\"})\n",
    "        \n",
    "        if df1.empty or df2.empty:\n",
    "            continue\n",
    "            \n",
    "        # Sort để merge_asof\n",
    "        df1 = df1.sort_values(\"time\")\n",
    "        df2 = df2.sort_values(\"time\")\n",
    "        \n",
    "        # MERGE TOÀN CỤC (Global Merge) - Nhanh hơn loop từng object\n",
    "        merged = pd.merge_asof(\n",
    "            df1, df2,\n",
    "            on=\"time\",\n",
    "            by=\"object_id\",   \n",
    "            direction=\"nearest\",\n",
    "            tolerance=1.0       \n",
    "        ).dropna()\n",
    "        \n",
    "        if merged.empty:\n",
    "            continue\n",
    "            \n",
    "        # Tính màu: Mag1 - Mag2\n",
    "        merged[\"color\"] = merged[\"mag1\"] - merged[\"mag2\"]\n",
    "        \n",
    "        # --- A. STATIC STATISTICS \n",
    "        # Tính Mean, Std, Min, Max cùng lúc\n",
    "        aggs = merged.groupby(\"object_id\")[\"color\"].agg(['mean', 'std', 'min', 'max'])\n",
    "        aggs.columns = [f\"color_{f1}_{f2}_{stat}\" for stat in aggs.columns]\n",
    "        aggs = aggs.reset_index()\n",
    "        \n",
    "        features = features.merge(aggs, on=\"object_id\", how=\"left\")\n",
    "        \n",
    "        # --- B. DYNAMICS (Slope & Change) - Cần Apply ---\n",
    "        def calc_dynamics(g):\n",
    "            if len(g) < 3: \n",
    "                return pd.Series([np.nan, np.nan], index=['slope', 'change'])\n",
    "            \n",
    "            # 1. Slope (Tốc độ thay đổi màu)\n",
    "            # TDE: Slope ~ 0 (Giữ nhiệt); SN: Slope > 0 (Nguội đi/Đỏ lên)\n",
    "            try:\n",
    "                # Normalize time\n",
    "                t = g[\"time\"].values - g[\"time\"].min()\n",
    "                c = g[\"color\"].values\n",
    "                slope = np.polyfit(t, c, 1)[0] \n",
    "            except:\n",
    "                slope = np.nan\n",
    "                \n",
    "            # 2. Total Change (Cuối - Đầu)\n",
    "            g_sorted = g.sort_values(\"time\")\n",
    "            # Lấy trung bình 2 điểm đầu/cuối để giảm nhiễu outlier\n",
    "            start_color = g_sorted[\"color\"].iloc[:2].mean()\n",
    "            end_color = g_sorted[\"color\"].iloc[-2:].mean()\n",
    "            change = end_color - start_color\n",
    "            \n",
    "            return pd.Series([slope, change], index=['slope', 'change'])\n",
    "\n",
    "        dynamics = merged.groupby(\"object_id\").apply(calc_dynamics)\n",
    "        dynamics.columns = [f\"color_{f1}_{f2}_slope\", f\"color_{f1}_{f2}_total_change\"]\n",
    "        dynamics = dynamics.reset_index()\n",
    "        \n",
    "        features = features.merge(dynamics, on=\"object_id\", how=\"left\")\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SHAPE FITTING PER FILTER \n",
    "# =========================\n",
    "\n",
    "def shape_fit_per_filter(df: pd.DataFrame, flux_col: str, cfg: Config) -> pd.DataFrame:\n",
    "    if not cfg.shape_fit_per_filter:\n",
    "        return pd.DataFrame({\"object_id\": df[\"object_id\"].unique()})\n",
    "    \n",
    "    err_col = \"Flux_err\"\n",
    "    \n",
    "    priority_filters = [\"g\", \"r\", \"i\"]\n",
    "    \n",
    "    tmp = df.copy()\n",
    "    # Tính SNR đúng\n",
    "    tmp[\"SNR\"] = tmp[flux_col].values / tmp[err_col].values\n",
    "    \n",
    "    # Get peak time per object \n",
    "    idx_peaks = tmp.groupby(\"object_id\")[flux_col].idxmax()\n",
    "    t_peak_per_obj = tmp.loc[idx_peaks, [\"object_id\", \"Time (MJD)\"]].set_index(\"object_id\")\n",
    "    \n",
    "    tmp = tmp.merge(t_peak_per_obj.rename(columns={\"Time (MJD)\": \"t_peak_obj\"}), on=\"object_id\", how=\"left\")\n",
    "    tmp[\"t_rel\"] = tmp[\"Time (MJD)\"] - tmp[\"t_peak_obj\"]\n",
    "    \n",
    "    all_feats = []\n",
    "    \n",
    "    for filt in priority_filters:\n",
    "        # Lọc data theo filter\n",
    "        filt_data = tmp[tmp[\"Filter\"] == filt].copy()\n",
    "        if len(filt_data) == 0: continue\n",
    "        \n",
    "        def per_obj_filt(g: pd.DataFrame) -> pd.Series:\n",
    "            mask_valid = (g[\"t_rel\"] > 0) & (g[\"t_rel\"] < 365) & (g[flux_col] > 0) #MAX_DECAY_WINDOW   \n",
    "            after = g[mask_valid].sort_values(\"t_rel\")\n",
    "            \n",
    "            pl_slope = np.nan\n",
    "            \n",
    "            # Cần tối thiểu 5 điểm để fit tin cậy\n",
    "            if len(after) >= 5:\n",
    "                t_fit = after[\"t_rel\"].values\n",
    "                f_fit = after[flux_col].values\n",
    "                e_fit = after[err_col].values\n",
    "                \n",
    "                try:\n",
    "                    log_t = np.log(t_fit)\n",
    "                    log_f = np.log(f_fit)\n",
    "\n",
    "                    weights = (f_fit / e_fit) ** 2\n",
    "\n",
    "                    coeffs = np.polyfit(log_t, log_f, 1, w=np.sqrt(weights))\n",
    "                    pl_slope = coeffs[0]\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return pd.Series({f\"{filt}_pl_slope\": pl_slope})\n",
    "        \n",
    "        # Apply group\n",
    "        filt_feats = filt_data.groupby(\"object_id\").apply(per_obj_filt).reset_index()\n",
    "        all_feats.append(filt_feats)\n",
    "    \n",
    "    # Merge results\n",
    "    if len(all_feats) == 0:\n",
    "        return pd.DataFrame({\"object_id\": df[\"object_id\"].unique()})\n",
    "    \n",
    "    result = all_feats[0]\n",
    "    for feat_df in all_feats[1:]:\n",
    "        result = result.merge(feat_df, on=\"object_id\", how=\"outer\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8247f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def variability_features(df: pd.DataFrame, flux_col: str, cfg: Config) -> pd.DataFrame:\n",
    "    \n",
    "    # 1. Chọn cột Error đúng\n",
    "    err_col = \"Flux_err\"\n",
    " \n",
    "    target_filters = ['g', 'r', 'i', 'z'] \n",
    "    \n",
    "    tmp = df.copy()\n",
    "    \n",
    "    all_feats = []\n",
    "\n",
    "    for filt in target_filters:\n",
    "        # Lọc dữ liệu theo filter\n",
    "        filt_df = tmp[tmp[\"Filter\"] == filt].copy()\n",
    "        \n",
    "        if len(filt_df) == 0: continue\n",
    "\n",
    "        def per_obj_filt(g: pd.DataFrame) -> pd.Series:\n",
    "            f = g[flux_col].values.astype(float)\n",
    "            e = g[err_col].values.astype(float)\n",
    "\n",
    "            mask = np.isfinite(f) & np.isfinite(e) & (e > 0)\n",
    "            f, e = f[mask], e[mask]\n",
    "            n = len(f)\n",
    "\n",
    "            if n < 3:\n",
    "                return pd.Series({\n",
    "                    f\"{filt}_stetson_j\": np.nan,\n",
    "                    f\"{filt}_von_neumann\": np.nan,\n",
    "                    f\"{filt}_entropy\": np.nan,\n",
    "                })\n",
    "\n",
    "            w_mean = np.average(f, weights=1.0 / (e**2))\n",
    "            delta = (f - w_mean) / e\n",
    "            stetson_j = float(np.sum(delta[:-1] * delta[1:]) / (n - 1))\n",
    "\n",
    "            diffs = np.diff(f)\n",
    "            mean_sq_diff = float(np.mean(diffs**2))\n",
    "            variance = float(np.var(f, ddof=1))\n",
    "            von_neumann = mean_sq_diff / variance if variance > 0 else np.nan\n",
    "\n",
    "            hist, _ = np.histogram(f, bins=10, density=False)\n",
    "            if hist.sum() > 0:\n",
    "                p = hist / hist.sum()\n",
    "                flux_entropy = float(entropy(p + 1e-12))\n",
    "            else:\n",
    "                flux_entropy = np.nan\n",
    "\n",
    "            return pd.Series({\n",
    "                f\"{filt}_stetson_j\": stetson_j,\n",
    "                f\"{filt}_von_neumann\": von_neumann,\n",
    "                f\"{filt}_entropy\": flux_entropy\n",
    "            })\n",
    "\n",
    "        # Groupby apply\n",
    "        filt_df = filt_df.sort_values([\"object_id\", \"Time (MJD)\"])\n",
    "        feat_df = filt_df.groupby(\"object_id\").apply(per_obj_filt).reset_index()\n",
    "        all_feats.append(feat_df)\n",
    "\n",
    "    # Merge lại\n",
    "    if not all_feats:\n",
    "         return pd.DataFrame({\"object_id\": df[\"object_id\"].unique()})\n",
    "\n",
    "    result = all_feats[0]\n",
    "    for f_df in all_feats[1:]:\n",
    "        result = result.merge(f_df, on=\"object_id\", how=\"outer\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93bc2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# METADATA FEATURES\n",
    "# =========================\n",
    "\n",
    "def metadata_features(log: pd.DataFrame, df_lc: pd.DataFrame, cfg: Config, is_train: bool) -> pd.DataFrame:\n",
    "\n",
    "    # 1. Prepare Metadata\n",
    "    keep_cols = [\"object_id\", \"Z\", \"EBV\"]\n",
    "    if \"target\" in log.columns and is_train:\n",
    "        keep_cols.append(\"target\")\n",
    "    # Giữ Z_err nếu có (quan trọng để đánh giá độ tin cậy của M_abs sau này)\n",
    "    if \"Z_err\" in log.columns:\n",
    "        keep_cols.append(\"Z_err\")\n",
    "\n",
    "    meta = log[keep_cols].copy()\n",
    "\n",
    "    valid_z = (meta[\"Z\"] > 1e-5) & np.isfinite(meta[\"Z\"])\n",
    "    meta[\"dL_Mpc\"] = np.nan\n",
    "    meta.loc[valid_z, \"dL_Mpc\"] = luminosity_distance_Mpc_fast(meta.loc[valid_z, \"Z\"].values)\n",
    "\n",
    "    lc = df_lc.copy()\n",
    "    flux_col = \"Flux\"\n",
    "    \n",
    "    peak_fluxes = lc.sort_values(flux_col, ascending=False).drop_duplicates([\"object_id\", \"Filter\"])\n",
    "    \n",
    "    # Pivot thành cột: peak_flux_u, peak_flux_g, ...\n",
    "    peak_pivot = peak_fluxes.pivot(index=\"object_id\", columns=\"Filter\", values=flux_col)\n",
    "    peak_pivot.columns = [f\"peak_flux_{f}\" for f in peak_pivot.columns]\n",
    "    \n",
    "    # Merge vào meta\n",
    "    meta = meta.merge(peak_pivot, on=\"object_id\", how=\"left\")\n",
    "\n",
    "    for f in ['g', 'r', 'u']: \n",
    "        col = f\"peak_flux_{f}\"\n",
    "        if col in meta.columns:\n",
    "            meta[f\"M_abs_{f}\"] = absolute_mag_from_flux_uJy(\n",
    "                meta[col].values, \n",
    "                meta[\"dL_Mpc\"].values\n",
    "            )\n",
    "        else:\n",
    "            meta[f\"M_abs_{f}\"] = np.nan\n",
    "\n",
    "    # 5. Derived Features\n",
    "    meta[\"log_Z\"] = np.log10(np.maximum(meta[\"Z\"], 1e-6)) # Safe log\n",
    "    meta[\"EBV_squared\"] = meta[\"EBV\"] ** 2\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_physics_features(df: pd.DataFrame, flux_col: str, cfg: Config) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Tính toán 4 đặc trưng vật lý cao cấp:\n",
    "    1. Late Flux Fraction (Đo lường độ tàn dư)\n",
    "    2. Multi-band Coherence (Sự đồng bộ giữa các dải màu)\n",
    "    3. Plateau Detection (Độ phẳng tại đỉnh)\n",
    "    4. Pre-explosion Variability (Độ ổn định trước khi nổ)\n",
    "    \"\"\"\n",
    "    tmp = df.copy()\n",
    "    \n",
    "    # Lọc nhiễu cơ bản để tính toán sạch hơn\n",
    "    # Chỉ lấy dữ liệu Valid\n",
    "    tmp = tmp.dropna(subset=[flux_col, \"Time (MJD)\"])\n",
    "    \n",
    "    # Chuẩn bị DataFrame kết quả\n",
    "    unique_ids = tmp[\"object_id\"].unique()\n",
    "    features = pd.DataFrame({\"object_id\": unique_ids})\n",
    "    \n",
    "    # Helper để tính correlation (Feature 2)\n",
    "    def calc_coherence(sub_df):\n",
    "        # Lấy band g và r (phổ biến nhất)\n",
    "        g = sub_df[sub_df[\"Filter\"] == 'g'][[\"Time (MJD)\", flux_col]].sort_values(\"Time (MJD)\")\n",
    "        r = sub_df[sub_df[\"Filter\"] == 'r'][[\"Time (MJD)\", flux_col]].sort_values(\"Time (MJD)\")\n",
    "        \n",
    "        if len(g) < 5 or len(r) < 5: return np.nan\n",
    "        \n",
    "        # Merge asof để ghép cặp thời gian gần nhất (trong vòng 1 ngày)\n",
    "        merged = pd.merge_asof(g, r, on=\"Time (MJD)\", direction='nearest', tolerance=1.0).dropna()\n",
    "        \n",
    "        if len(merged) < 5: return np.nan\n",
    "        \n",
    "        # Chuẩn hóa (Normalize)\n",
    "        g_norm = (merged[f\"{flux_col}_x\"] - merged[f\"{flux_col}_x\"].mean()) / (merged[f\"{flux_col}_x\"].std() + 1e-6)\n",
    "        r_norm = (merged[f\"{flux_col}_y\"] - merged[f\"{flux_col}_y\"].mean()) / (merged[f\"{flux_col}_y\"].std() + 1e-6)\n",
    "        \n",
    "        # Tính Correlation\n",
    "        try:\n",
    "            return np.corrcoef(g_norm, r_norm)[0, 1]\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    # Helper tính 3 feature còn lại (dựa trên Peak)\n",
    "    def calc_peak_rel_features(g):\n",
    "        # Tìm đỉnh\n",
    "        if g[flux_col].max() <= 0: return pd.Series([np.nan]*3, index=['late_flux_ratio', 'plateau_score', 'pre_burst_std'])\n",
    "        \n",
    "        idx_max = g[flux_col].idxmax()\n",
    "        t_peak = g.loc[idx_max, \"Time (MJD)\"]\n",
    "        f_peak = g.loc[idx_max, flux_col]\n",
    "        \n",
    "        # Calculate t_rel\n",
    "        t_rel = g[\"Time (MJD)\"] - t_peak\n",
    "        \n",
    "        # --- 1. LATE FLUX FRACTION (Tail Energy) ---\n",
    "        # TDE: Vẫn còn sáng sau 50-100 ngày\n",
    "        # SN: Tắt ngấm\n",
    "        late_mask = (t_rel > 50) & (t_rel < 200) # Cửa sổ 50-200 ngày sau đỉnh\n",
    "        late_flux = g.loc[late_mask, flux_col]\n",
    "        \n",
    "        if len(late_flux) > 0:\n",
    "            # Lấy median của đuôi chia cho đỉnh\n",
    "            late_ratio = late_flux.median() / f_peak\n",
    "        else:\n",
    "            late_ratio = 0.0 # Không có dữ liệu đuôi coi như tắt ngấm\n",
    "            \n",
    "        # --- 3. PLATEAU DETECTION ---\n",
    "        # TDE/SN IIp: Có giai đoạn phẳng quanh đỉnh\n",
    "        # SN Ia: Nhọn hoắt\n",
    "        # Cửa sổ: +/- 15 ngày quanh đỉnh\n",
    "        plat_mask = (t_rel > -15) & (t_rel < 15)\n",
    "        plat_flux = g.loc[plat_mask, flux_col]\n",
    "        \n",
    "        if len(plat_flux) > 3:\n",
    "            # Coefficient of Variation (CV) = Std / Mean\n",
    "            # Nếu phẳng -> Std thấp -> Score thấp\n",
    "            plateau_score = plat_flux.std() / (plat_flux.mean() + 1e-6)\n",
    "        else:\n",
    "            plateau_score = np.nan\n",
    "            \n",
    "        # --- 4. PRE-EXPLOSION VARIABILITY ---\n",
    "        # TDE: Yên tĩnh tuyệt đối trước khi nổ (Baseline Subtracted ~ 0)\n",
    "        # AGN: Biến động liên tục\n",
    "        pre_mask = (t_rel < -30) # Trước đỉnh 30 ngày\n",
    "        pre_flux = g.loc[pre_mask, flux_col]\n",
    "        \n",
    "        if len(pre_flux) > 3:\n",
    "            pre_burst_std = pre_flux.std()\n",
    "        else:\n",
    "            pre_burst_std = np.nan\n",
    "            \n",
    "        return pd.Series({\n",
    "            'late_flux_ratio': late_ratio,\n",
    "            'plateau_score': plateau_score,\n",
    "            'pre_burst_std': pre_burst_std\n",
    "        })\n",
    "\n",
    "    # --- THỰC THI (Apply) ---\n",
    "    # 1. Coherence (Tính trên toàn cục object, không cần groupby peak)\n",
    "    print(\"   -> Calculating Multi-band Coherence...\")\n",
    "    coherence = tmp.groupby(\"object_id\").apply(calc_coherence).reset_index(name=\"multiband_coherence\")\n",
    "    features = features.merge(coherence, on=\"object_id\", how=\"left\")\n",
    "    \n",
    "    # 2. Peak-based features\n",
    "    print(\"   -> Calculating Peak-based Physics features...\")\n",
    "    # Group và apply hàm tính 3 chỉ số kia\n",
    "    peak_feats = tmp.groupby(\"object_id\").apply(calc_peak_rel_features).reset_index()\n",
    "    features = features.merge(peak_feats, on=\"object_id\", how=\"left\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# IMPUTATION + MISSING \n",
    "# =========================\n",
    "\n",
    "def impute_features(train_df: pd.DataFrame, test_df: pd.DataFrame, \n",
    "                              target_col: str = \"target\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    exclude = [\"object_id\", \"split\", \"dataset\"]\n",
    "    if target_col in train_df.columns:\n",
    "        exclude.append(target_col)\n",
    "\n",
    "    all_cols = sorted(set(train_df.columns) | set(test_df.columns))\n",
    "    \n",
    "    train_df = train_df.reindex(columns=all_cols)\n",
    "    test_df  = test_df.reindex(columns=all_cols)\n",
    "    \n",
    "    # Xác định danh sách feature (loại bỏ ID và Target)\n",
    "    feat_cols = [c for c in all_cols if c not in exclude]\n",
    "\n",
    "    train_df[feat_cols] = train_df[feat_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    test_df[feat_cols]  = test_df[feat_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    count_like_cols = [c for c in feat_cols if (\n",
    "        c.startswith(\"n_\") or           # n_peaks, n_obs...\n",
    "        c.startswith(\"number_\") or\n",
    "        \"count\" in c or\n",
    "        \"has_\" in c or                  # has_clear_peak...\n",
    "        \"_reliable\" in c or             # decay_reliable...\n",
    "        \"is_\" in c                      # is_missing...\n",
    "    )]\n",
    "    \n",
    "    if count_like_cols:\n",
    "        train_df[count_like_cols] = train_df[count_like_cols].fillna(0)\n",
    "        test_df[count_like_cols]  = test_df[count_like_cols].fillna(0)\n",
    "        logger.info(f\"  Filled 0 for {len(count_like_cols)} count/logic columns.\")\n",
    "\n",
    "    critical_patterns = [\n",
    "        \"decay_discrimination\", \n",
    "        \"_med\",    \n",
    "        \"rise_time\",       \n",
    "        \"_slope\",\n",
    "        \"pl_slope\",\n",
    "        \"M_abs\",\n",
    "        \"color_\",      \n",
    "        \"_r2\",          \n",
    "        \"_stetson\",     \n",
    "        \"peak_snr\"     \n",
    "    ]\n",
    "    \n",
    "    added_indicators = []\n",
    "    \n",
    "    for pattern in critical_patterns:\n",
    "        matching_cols = [c for c in feat_cols if pattern in c]\n",
    "        \n",
    "        for col in matching_cols:\n",
    "            if col in train_df.columns:\n",
    "                # Only add if there ARE missing values\n",
    "                if train_df[col].isna().any() or test_df[col].isna().any():\n",
    "                    ind_name = f\"missing_{col}\"\n",
    "                    train_df[ind_name] = train_df[col].isna().astype(int)\n",
    "                    test_df[ind_name] = test_df[col].isna().astype(int)\n",
    "                    added_indicators.append(ind_name)\n",
    "    \n",
    "    logger.info(f\"  ✓ Added {len(added_indicators)} missing indicators\")\n",
    "\n",
    "    \n",
    "    remaining_nans = train_df[feat_cols].isna().sum().sum()\n",
    "    logger.info(f\"  ✓ Kept {remaining_nans} NaNs for LightGBM native handling\")\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45cadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def feature_selection_cv_optimized(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    cfg: Config,\n",
    "    target_col: str = \"target\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    FULLY FIXED & PRODUCTION-READY\n",
    "    ================================\n",
    "    1. Variance filter\n",
    "    2. Smart correlation removal\n",
    "    3. LightGBM CV importance\n",
    "    4. Safe train/test reconstruction\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(\"Starting FIXED feature selection...\")\n",
    "    \n",
    "    id_cols = [\"object_id\", \"split\", \"dataset\"]\n",
    "    drop_cols = [c for c in id_cols + [target_col] if c in train_df.columns]\n",
    "    \n",
    "    X = train_df.drop(columns=drop_cols).copy()\n",
    "    y = train_df[target_col].values.astype(int)\n",
    "    \n",
    "    # ==================================================\n",
    "    # 1. Variance Threshold\n",
    "    # ==================================================\n",
    "    vt = VarianceThreshold(threshold=0.0)\n",
    "    mask = vt.fit(X).get_support()\n",
    "    X = X.loc[:, mask]\n",
    "    logger.info(f\"  After variance filter: {X.shape[1]} features\")\n",
    "    \n",
    "    # ==================================================\n",
    "    # 2. SMART Correlation Filter\n",
    "    # ==================================================\n",
    "    logger.info(f\"  Removing correlated features (>{cfg.corr_threshold})...\")\n",
    "    \n",
    "    # Compute correlation efficiently\n",
    "    X_temp = X.copy()\n",
    "    X_temp['__target__'] = y\n",
    "    corr_full = X_temp.corr().abs()\n",
    "    \n",
    "    corr_matrix = corr_full.drop('__target__', axis=1).drop('__target__', axis=0)\n",
    "    corr_with_target = corr_full['__target__'].drop('__target__')\n",
    "    \n",
    "    upper = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    to_drop = set()\n",
    "    \n",
    "    for col in upper.columns:\n",
    "        if col in to_drop:\n",
    "            continue\n",
    "        \n",
    "        high_corr_cols = upper.index[upper[col] > cfg.corr_threshold].tolist()\n",
    "        \n",
    "        for base_col in high_corr_cols:\n",
    "            if base_col in to_drop:\n",
    "                continue\n",
    "            \n",
    "            if corr_with_target[col] >= corr_with_target[base_col]:\n",
    "                to_drop.add(base_col)\n",
    "            else:\n",
    "                to_drop.add(col)\n",
    "                break\n",
    "    \n",
    "    X = X.drop(columns=list(to_drop))\n",
    "    logger.info(f\"    → {X.shape[1]} features (dropped {len(to_drop)})\")\n",
    "    \n",
    "    # ==================================================\n",
    "    # 3. LightGBM Importance\n",
    "    # ==================================================\n",
    "    logger.info(\"  Computing LightGBM importance with 5-fold CV...\")\n",
    "    \n",
    "    feature_names = X.columns.tolist()\n",
    "    importances = np.zeros(len(feature_names))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=cfg.random_state)\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            random_state=cfg.random_state + fold,\n",
    "            n_jobs=-1,\n",
    "            is_unbalance=True,\n",
    "            verbose=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_va, y_va)],\n",
    "            callbacks=[lgb.early_stopping(30, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        importances += model.booster_.feature_importance(importance_type=\"gain\") / 5.0\n",
    "    \n",
    "    imp_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance\": importances\n",
    "    }).sort_values(\"importance\", ascending=False)\n",
    "    \n",
    "    top_features = imp_df.head(cfg.rf_top_k)[\"feature\"].tolist()\n",
    "    \n",
    "    logger.info(f\"  Selected top {len(top_features)} features\")\n",
    "    logger.info(f\"  Top 5: {top_features[:5]}\")\n",
    "    \n",
    "    # Step 4a: Align test columns with X\n",
    "    test_X = test_df.drop(columns=[c for c in id_cols if c in test_df.columns]).copy()\n",
    "    \n",
    "    # Handle missing columns\n",
    "    missing_in_test = set(X.columns) - set(test_X.columns)\n",
    "    if missing_in_test:\n",
    "        logger.warning(f\"  ⚠ Test missing {len(missing_in_test)} cols, filling 0\")\n",
    "        for col in missing_in_test:\n",
    "            test_X[col] = 0\n",
    "    \n",
    "    # Handle extra columns\n",
    "    extra_in_test = set(test_X.columns) - set(X.columns)\n",
    "    if extra_in_test:\n",
    "        logger.warning(f\"  ⚠ Test has {len(extra_in_test)} extra cols, dropping\")\n",
    "    \n",
    "    # Ensure same order\n",
    "    test_X = test_X[X.columns]\n",
    "    \n",
    "    # Step 4b: Select top features\n",
    "    X_selected = X[top_features].reset_index(drop=True)\n",
    "    test_X_selected = test_X[top_features].reset_index(drop=True)\n",
    "    \n",
    "    # Step 4c: Merge metadata back\n",
    "    train_meta = train_df[id_cols + [target_col]].reset_index(drop=True)\n",
    "    test_meta = test_df[id_cols].reset_index(drop=True)\n",
    "    \n",
    "    train_out = pd.concat([train_meta, X_selected], axis=1)\n",
    "    test_out = pd.concat([test_meta, test_X_selected], axis=1)\n",
    "    \n",
    "    # Verify\n",
    "    assert len(train_out) == len(train_df), \"Row count mismatch!\"\n",
    "    assert len(test_out) == len(test_df), \"Row count mismatch!\"\n",
    "    assert set(train_out.columns) == set(id_cols + [target_col] + top_features)\n",
    "    \n",
    "    return train_out, test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1e11c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(cfg: Config, split_idx: int, mode: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load lightcurves and log for one split.\"\"\"\n",
    "    split_name = f\"split_{split_idx:02d}\"\n",
    "    split_dir = f\"cleaned_data/{split_name}\"\n",
    "\n",
    "    lc_path =  f\"{split_dir}_{mode}_processed.csv\"\n",
    "    log_path = f\"{mode}_log.csv\"  # Log is at root level\n",
    "\n",
    "    if not os.path.exists(lc_path):\n",
    "        raise FileNotFoundError(f\"Lightcurve file not found: {lc_path}\")\n",
    "    if not os.path.exists(log_path):\n",
    "        raise FileNotFoundError(f\"Log file not found: {log_path}\")\n",
    "\n",
    "    lc = pd.read_csv(lc_path)\n",
    "    log = pd.read_csv(log_path)\n",
    "\n",
    "    return lc, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "217f7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "lc_train, log_train = load_file(cfg, split_idx=1, mode=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13694fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features_for_split(cfg: Config, split_idx: int, mode: str) -> pd.DataFrame:\n",
    "    is_train = (mode == \"train\")\n",
    "    \n",
    "    logger.info(f\"{'='*60}\")\n",
    "    logger.info(f\"Processing split_{split_idx:02d} - {mode.upper()}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "\n",
    "    # Load\n",
    "    lc, log_full = load_file(cfg, split_idx, mode)\n",
    "\n",
    "\n",
    "    # Merge metadata\n",
    "    merge_cols = [\"object_id\", \"Z\", \"EBV\"]\n",
    "    if (not is_train) and (\"Z_err\" in log_full.columns):\n",
    "        merge_cols.append(\"Z_err\")\n",
    "    if is_train and (\"target\" in log_full.columns):\n",
    "        merge_cols.append(\"target\")\n",
    "\n",
    "    lc = lc.merge(log_full[merge_cols], on=\"object_id\", how=\"left\")\n",
    "\n",
    "    flux_col = \"Flux\"\n",
    "\n",
    "    # ===== LEVEL 1: Metadata =====\n",
    "    logger.info(\"Extracting Level 1: Metadata features...\")\n",
    "    meta_df_source = lc.groupby(\"object_id\").first().reset_index()\n",
    "    meta = metadata_features(meta_df_source, lc, cfg, is_train=is_train)\n",
    "\n",
    "    # ===== LEVEL 2: Statistical =====\n",
    "    logger.info(\"Extracting Level 2: Statistical features per filter...\")\n",
    "    counts = count_features_per_filter(lc, cfg)\n",
    "    stats_flux = stats_features_per_filter_enhanced(lc, flux_col, cfg)\n",
    "    stats_err  = stats_features_per_filter_enhanced(lc, \"Flux_err\", cfg)\n",
    "\n",
    "    # ===== LEVEL 3: Time-domain =====\n",
    "    logger.info(\"Extracting Level 3: Time-domain features...\")\n",
    "    snr = snr_features(lc, flux_col, cfg)\n",
    "    time_overall = overall_time_features_enhanced(lc, flux_col, cfg)\n",
    "\n",
    "    # ===== LEVEL 4: Rise/Decay + Power-law =====\n",
    "    logger.info(\"Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\")\n",
    "    rise_decay = rise_decay_features_powerlaw(lc, flux_col, cfg)\n",
    "\n",
    "    # ===== LEVEL 5: Variability indices =====\n",
    "    logger.info(\"Extracting Level 5: Variability indices...\")\n",
    "    variability = variability_features(lc, flux_col, cfg)\n",
    "\n",
    "    # ===== LEVEL 6: Multi-band colors (UNIFIED) =====\n",
    "    logger.info(\"Extracting Level 6: Unified Color features (Stats + Evolution)...\")\n",
    "    \n",
    "    # Gọi hàm duy nhất này thay vì 2 hàm cũ\n",
    "    colors_unified = color_features_unified(lc, flux_col, cfg)  \n",
    "\n",
    "\n",
    "    # ===== LEVEL 7: Per-filter shape fitting =====\n",
    "    if cfg.shape_fit_per_filter:\n",
    "        logger.info(\"Extracting Level 7: Per-filter shape fitting...\")\n",
    "        shape_per_filt = shape_fit_per_filter(lc, flux_col, cfg)\n",
    "    else:\n",
    "        shape_per_filt = pd.DataFrame({\"object_id\": lc[\"object_id\"].unique()})\n",
    "\n",
    "    \n",
    "    # ===== Combine all =====\n",
    "    logger.info(\"Combining all features...\")\n",
    "    feats = meta.copy()\n",
    "    \n",
    "    for name, part in [\n",
    "        (\"counts\", counts),\n",
    "        (\"stats_flux\", stats_flux),\n",
    "        (\"stats_err\", stats_err),\n",
    "        (\"snr\", snr),\n",
    "        (\"time_overall\", time_overall),\n",
    "        (\"rise_decay\", rise_decay),\n",
    "        (\"variability\", variability),\n",
    "        (\"colors\", colors_unified),\n",
    "        (\"shape_per_filt\", shape_per_filt)\n",
    "    ]:\n",
    "        feats = feats.merge(part, on=\"object_id\", how=\"left\")\n",
    "        logger.info(f\"  Merged {name}: {part.shape[1]-1} features\")\n",
    "\n",
    "    feats[\"split\"] = f\"split_{split_idx:02d}\"\n",
    "    feats[\"dataset\"] = mode\n",
    "\n",
    "   \n",
    "\n",
    "    logger.info(f\"✓ Completed split_{split_idx:02d}: {len(feats)} objects, {len(feats.columns)} features\")\n",
    "\n",
    "    print(feats[\"object_id\"].nunique())\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# MAIN: ALL SPLITS\n",
    "# =========================\n",
    "\n",
    "def run_all(cfg: Config) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    os.makedirs(cfg.out_root, exist_ok=True)\n",
    "    \n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"TDE FEATURE ENGINEERING - COMPLETE PIPELINE\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"Configuration:\")\n",
    "    logger.info(f\"  Data root: {cfg.data_root}\")\n",
    "    logger.info(f\"  Output root: {cfg.out_root}\")\n",
    "    logger.info(f\"  Splits: {cfg.splits[0]} to {cfg.splits[1]}\")\n",
    "    logger.info(f\"  Shape fitting: {cfg.do_shape_fitting}\")\n",
    "    logger.info(f\"  Per-filter fitting: {cfg.shape_fit_per_filter}\")\n",
    "    logger.info(f\"  Feature selection: {cfg.do_feature_selection}\")\n",
    "    logger.info(f\"  Multiprocessing: {cfg.use_multiprocessing}\")\n",
    "    logger.info(\"=\"*80)\n",
    "\n",
    "    if cfg.use_multiprocessing:\n",
    "        # Parallel processing\n",
    "        from multiprocessing import Pool\n",
    "        \n",
    "        logger.info(f\"Using multiprocessing with {cfg.n_jobs} workers...\")\n",
    "        \n",
    "        with Pool(processes=cfg.n_jobs) as pool:\n",
    "            train_args = [(cfg, s, \"train\") for s in range(cfg.splits[0], cfg.splits[1] + 1)]\n",
    "            test_args = [(cfg, s, \"test\") for s in range(cfg.splits[0], cfg.splits[1] + 1)]\n",
    "            \n",
    "            train_parts = pool.starmap(build_features_for_split, train_args)\n",
    "            test_parts = pool.starmap(build_features_for_split, test_args)\n",
    "    else:\n",
    "        # Sequential processing\n",
    "        train_parts = []\n",
    "        test_parts = []\n",
    "\n",
    "        for s in range(cfg.splits[0], cfg.splits[1] + 1):\n",
    "            tr = build_features_for_split(cfg, s, \"train\")\n",
    "            te = build_features_for_split(cfg, s, \"test\")\n",
    "\n",
    "            train_parts.append(tr)\n",
    "            test_parts.append(te)\n",
    "\n",
    "    # Concatenate\n",
    "    logger.info(\"Concatenating all splits...\")\n",
    "    train_df = pd.concat(train_parts, ignore_index=True)\n",
    "    test_df = pd.concat(test_parts, ignore_index=True)\n",
    "    \n",
    "    logger.info(f\"  Train: {len(train_df)} objects, {len(train_df.columns)} features\")\n",
    "    logger.info(f\"  Test:  {len(test_df)} objects, {len(test_df.columns)} features\")\n",
    "\n",
    "    # Imputation\n",
    "    if cfg.do_imputation:\n",
    "        logger.info(\"Applying imputation...\")\n",
    "        train_df, test_df = impute_features(train_df, test_df, target_col=\"target\")\n",
    "\n",
    "    # Feature selection\n",
    "    if cfg.do_feature_selection:\n",
    "        logger.info(\"Applying feature selection...\")\n",
    "        train_df, test_df = feature_selection_cv_optimized(train_df, test_df, cfg, target_col=\"target\")\n",
    "\n",
    "    # Save\n",
    "    train_path = os.path.join(cfg.out_root, \"train_features_all_splits.csv\")\n",
    "    test_path = os.path.join(cfg.out_root, \"test_features_all_splits.csv\")\n",
    "    \n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(\"FEATURE ENGINEERING COMPLETE!\")\n",
    "    logger.info(\"=\"*80)\n",
    "    logger.info(f\"✓ Train features: {train_path}\")\n",
    "    logger.info(f\"  - {len(train_df)} objects\")\n",
    "    logger.info(f\"  - {len(train_df.columns)} total columns\")\n",
    "    logger.info(f\"  - {len([c for c in train_df.columns if c not in ['object_id', 'split', 'dataset', 'target']])} feature columns\")\n",
    "    logger.info(f\"✓ Test features: {test_path}\")\n",
    "    logger.info(f\"  - {len(test_df)} objects\")\n",
    "    logger.info(f\"  - {len(test_df.columns)} total columns\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    if \"target\" in train_df.columns:\n",
    "        target_dist = train_df[\"target\"].value_counts()\n",
    "        logger.info(f\"\\nTarget distribution:\")\n",
    "        logger.info(f\"  Non-TDE (0): {target_dist.get(0, 0)}\")\n",
    "        logger.info(f\"  TDE (1):     {target_dist.get(1, 0)}\")\n",
    "        logger.info(f\"  Ratio:       1:{target_dist.get(0, 0)/target_dist.get(1, 1):.1f}\")\n",
    "    \n",
    "    logger.info(\"=\"*80)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0f7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:30:40,263 - INFO - Starting feature engineering with OPTIMIZED configuration...\n",
      "2025-12-23 00:30:40,265 - INFO - Key improvements:\n",
      "2025-12-23 00:30:40,266 - INFO -   ✓ Power-law vs exponential decay discrimination\n",
      "2025-12-23 00:30:40,267 - INFO -   ✓ UV excess (u-g color)\n",
      "2025-12-23 00:30:40,268 - INFO -   ✓ Color evolution rates\n",
      "2025-12-23 00:30:40,269 - INFO -   ✓ Per-filter shape fitting\n",
      "2025-12-23 00:30:40,270 - INFO -   ✓ Enhanced statistical features (skew, kurt, MAD)\n",
      "2025-12-23 00:30:40,271 - INFO -   ✓ Variability indices (Stetson J, Von Neumann)\n",
      "2025-12-23 00:30:40,272 - INFO -   ✓ Improved missing data handling\n",
      "2025-12-23 00:30:40,273 - INFO -   ✓ CV-based feature selection\n",
      "2025-12-23 00:30:40,275 - INFO - ================================================================================\n",
      "2025-12-23 00:30:40,276 - INFO - TDE FEATURE ENGINEERING - COMPLETE PIPELINE\n",
      "2025-12-23 00:30:40,277 - INFO - ================================================================================\n",
      "2025-12-23 00:30:40,278 - INFO - Configuration:\n",
      "2025-12-23 00:30:40,278 - INFO -   Data root: \n",
      "2025-12-23 00:30:40,279 - INFO -   Output root: features_out_data_feature_selection\n",
      "2025-12-23 00:30:40,280 - INFO -   Splits: 1 to 20\n",
      "2025-12-23 00:30:40,281 - INFO -   Shape fitting: True\n",
      "2025-12-23 00:30:40,282 - INFO -   Per-filter fitting: True\n",
      "2025-12-23 00:30:40,282 - INFO -   Feature selection: True\n",
      "2025-12-23 00:30:40,283 - INFO -   Multiprocessing: False\n",
      "2025-12-23 00:30:40,284 - INFO - ================================================================================\n",
      "2025-12-23 00:30:40,285 - INFO - ============================================================\n",
      "2025-12-23 00:30:40,286 - INFO - Processing split_01 - TRAIN\n",
      "2025-12-23 00:30:40,287 - INFO - ============================================================\n",
      "2025-12-23 00:30:40,353 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:30:40,406 - INFO - Extracting Level 2: Statistical features per filter...\n",
      "2025-12-23 00:30:44,928 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:30:45,724 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:30:47,502 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:30:50,441 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:30:51,588 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:30:53,275 - INFO - Combining all features...\n",
      "2025-12-23 00:30:53,287 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:30:53,297 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:30:53,312 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:30:53,323 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:30:53,337 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:30:53,347 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:30:53,361 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:30:53,372 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:30:53,386 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:30:53,391 - INFO - ✓ Completed split_01: 155 objects, 248 features\n",
      "2025-12-23 00:30:53,401 - INFO - ============================================================\n",
      "2025-12-23 00:30:53,404 - INFO - Processing split_01 - TEST\n",
      "2025-12-23 00:30:53,407 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:30:53,760 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:30:53,958 - INFO - Extracting Level 2: Statistical features per filter...\n",
      "2025-12-23 00:31:08,707 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:31:09,377 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:31:11,618 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:31:14,363 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:31:17,247 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:31:19,288 - INFO - Combining all features...\n",
      "2025-12-23 00:31:19,295 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:31:19,301 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:31:19,307 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:31:19,313 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:31:19,320 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:31:19,340 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:31:19,361 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:31:19,380 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:31:19,420 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:31:19,427 - INFO - ✓ Completed split_01: 364 objects, 248 features\n",
      "2025-12-23 00:31:19,458 - INFO - ============================================================\n",
      "2025-12-23 00:31:19,462 - INFO - Processing split_02 - TRAIN\n",
      "2025-12-23 00:31:19,476 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:31:19,642 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:31:19,723 - INFO - Extracting Level 2: Statistical features per filter...\n",
      "2025-12-23 00:31:28,104 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:31:28,419 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:31:29,173 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:31:30,403 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:31:31,005 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:31:31,937 - INFO - Combining all features...\n",
      "2025-12-23 00:31:31,943 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:31:31,949 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:31:31,954 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:31:31,961 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:31:31,967 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:31:31,973 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:31:31,980 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:31:31,986 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:31:31,993 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:31:31,996 - INFO - ✓ Completed split_02: 170 objects, 248 features\n",
      "2025-12-23 00:31:32,000 - INFO - ============================================================\n",
      "2025-12-23 00:31:32,001 - INFO - Processing split_02 - TEST\n",
      "2025-12-23 00:31:32,003 - INFO - ============================================================\n",
      "2025-12-23 00:31:32,188 - INFO - Extracting Level 1: Metadata features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:31:32,345 - INFO - Extracting Level 2: Statistical features per filter...\n",
      "2025-12-23 00:31:51,972 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:31:52,851 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:31:54,705 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:31:57,825 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:31:58,964 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:32:01,458 - INFO - Combining all features...\n",
      "2025-12-23 00:32:01,463 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:32:01,467 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:32:01,474 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:32:01,479 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:32:01,485 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:32:01,492 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:32:01,496 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:32:01,502 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:32:01,508 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:32:01,511 - INFO - ✓ Completed split_02: 414 objects, 248 features\n",
      "2025-12-23 00:32:01,518 - INFO - ============================================================\n",
      "2025-12-23 00:32:01,519 - INFO - Processing split_03 - TRAIN\n",
      "2025-12-23 00:32:01,521 - INFO - ============================================================\n",
      "2025-12-23 00:32:01,596 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:32:01,651 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:32:08,469 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:32:08,772 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:32:09,373 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:32:10,370 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:32:10,813 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:32:11,570 - INFO - Combining all features...\n",
      "2025-12-23 00:32:11,574 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:32:11,580 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:32:11,585 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:32:11,590 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:32:11,595 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:32:11,599 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:32:11,607 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:32:11,612 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:32:11,619 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:32:11,622 - INFO - ✓ Completed split_03: 138 objects, 248 features\n",
      "2025-12-23 00:32:11,625 - INFO - ============================================================\n",
      "2025-12-23 00:32:11,626 - INFO - Processing split_03 - TEST\n",
      "2025-12-23 00:32:11,628 - INFO - ============================================================\n",
      "2025-12-23 00:32:11,783 - INFO - Extracting Level 1: Metadata features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:32:11,908 - INFO - Extracting Level 2: Statistical features per filter...\n",
      "2025-12-23 00:32:28,971 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:32:29,617 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:32:31,075 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:32:33,621 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:32:34,629 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:32:36,509 - INFO - Combining all features...\n",
      "2025-12-23 00:32:36,514 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:32:36,519 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:32:36,525 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:32:36,532 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:32:36,537 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:32:36,543 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:32:36,550 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:32:36,555 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:32:36,561 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:32:36,563 - INFO - ✓ Completed split_03: 338 objects, 248 features\n",
      "2025-12-23 00:32:36,566 - INFO - ============================================================\n",
      "2025-12-23 00:32:36,567 - INFO - Processing split_04 - TRAIN\n",
      "2025-12-23 00:32:36,568 - INFO - ============================================================\n",
      "2025-12-23 00:32:36,642 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:32:36,707 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:32:43,864 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:32:44,149 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:32:44,828 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:32:45,930 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:32:46,449 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:32:47,283 - INFO - Combining all features...\n",
      "2025-12-23 00:32:47,287 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:32:47,291 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:32:47,297 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:32:47,303 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:32:47,309 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:32:47,315 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:32:47,321 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:32:47,327 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:32:47,333 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:32:47,337 - INFO - ✓ Completed split_04: 145 objects, 248 features\n",
      "2025-12-23 00:32:47,342 - INFO - ============================================================\n",
      "2025-12-23 00:32:47,344 - INFO - Processing split_04 - TEST\n",
      "2025-12-23 00:32:47,346 - INFO - ============================================================\n",
      "2025-12-23 00:32:47,484 - INFO - Extracting Level 1: Metadata features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:32:47,604 - INFO - Extracting Level 2: Statistical features per filter...\n",
      "2025-12-23 00:33:03,766 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:33:04,366 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:33:05,738 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:33:08,341 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:33:09,273 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:33:10,955 - INFO - Combining all features...\n",
      "2025-12-23 00:33:10,961 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:33:10,966 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:33:10,971 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:33:10,978 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:33:10,984 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:33:10,990 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:33:10,998 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:33:11,006 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:33:11,014 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:33:11,018 - INFO - ✓ Completed split_04: 332 objects, 248 features\n",
      "2025-12-23 00:33:11,024 - INFO - ============================================================\n",
      "2025-12-23 00:33:11,027 - INFO - Processing split_05 - TRAIN\n",
      "2025-12-23 00:33:11,030 - INFO - ============================================================\n",
      "2025-12-23 00:33:11,111 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:33:11,166 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:33:18,928 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:33:19,233 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:33:19,956 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:33:21,126 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:33:21,714 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:33:22,585 - INFO - Combining all features...\n",
      "2025-12-23 00:33:22,590 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:33:22,595 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:33:22,601 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:33:22,607 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:33:22,613 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:33:22,618 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:33:22,625 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:33:22,630 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:33:22,636 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:33:22,639 - INFO - ✓ Completed split_05: 165 objects, 248 features\n",
      "2025-12-23 00:33:22,643 - INFO - ============================================================\n",
      "2025-12-23 00:33:22,644 - INFO - Processing split_05 - TEST\n",
      "2025-12-23 00:33:22,645 - INFO - ============================================================\n",
      "2025-12-23 00:33:22,811 - INFO - Extracting Level 1: Metadata features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:33:22,937 - INFO - Extracting Level 2: Statistical features per filter...\n",
      "2025-12-23 00:33:40,458 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:33:41,126 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:33:42,732 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:33:45,314 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:33:46,366 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:33:48,368 - INFO - Combining all features...\n",
      "2025-12-23 00:33:48,373 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:33:48,378 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:33:48,384 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:33:48,393 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:33:48,399 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:33:48,405 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:33:48,410 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:33:48,417 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:33:48,425 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:33:48,427 - INFO - ✓ Completed split_05: 375 objects, 248 features\n",
      "2025-12-23 00:33:48,436 - INFO - ============================================================\n",
      "2025-12-23 00:33:48,437 - INFO - Processing split_06 - TRAIN\n",
      "2025-12-23 00:33:48,439 - INFO - ============================================================\n",
      "2025-12-23 00:33:48,518 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:33:48,580 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:33:55,849 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:33:56,138 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:33:56,789 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:33:57,855 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:33:58,302 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:33:59,167 - INFO - Combining all features...\n",
      "2025-12-23 00:33:59,172 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:33:59,177 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:33:59,183 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:33:59,188 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:33:59,194 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:33:59,200 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:33:59,206 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:33:59,211 - INFO -   Merged colors: 30 features\n",
      "2025-12-23 00:33:59,215 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:33:59,218 - INFO - ✓ Completed split_06: 155 objects, 242 features\n",
      "2025-12-23 00:33:59,222 - INFO - ============================================================\n",
      "2025-12-23 00:33:59,223 - INFO - Processing split_06 - TEST\n",
      "2025-12-23 00:33:59,223 - INFO - ============================================================\n",
      "2025-12-23 00:33:59,383 - INFO - Extracting Level 1: Metadata features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:33:59,503 - INFO - Extracting Level 2: Statistical features per filter...\n",
      "2025-12-23 00:34:17,301 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:34:17,978 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:34:19,795 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:34:22,252 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:34:22,510 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:34:23,037 - INFO - Combining all features...\n",
      "2025-12-23 00:34:23,039 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:34:23,040 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:34:23,042 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:34:23,044 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:34:23,046 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:34:23,048 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:34:23,050 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:34:23,052 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:34:23,054 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:34:23,055 - INFO - ✓ Completed split_06: 374 objects, 248 features\n",
      "2025-12-23 00:34:23,058 - INFO - ============================================================\n",
      "2025-12-23 00:34:23,059 - INFO - Processing split_07 - TRAIN\n",
      "2025-12-23 00:34:23,059 - INFO - ============================================================\n",
      "2025-12-23 00:34:23,082 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:34:23,097 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:34:24,755 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:34:24,823 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:34:25,014 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:34:25,334 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:34:25,467 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:34:25,705 - INFO - Combining all features...\n",
      "2025-12-23 00:34:25,707 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:34:25,708 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:34:25,710 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:34:25,711 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:34:25,713 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:34:25,715 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:34:25,717 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:34:25,721 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:34:25,722 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:34:25,723 - INFO - ✓ Completed split_07: 165 objects, 248 features\n",
      "2025-12-23 00:34:25,725 - INFO - ============================================================\n",
      "2025-12-23 00:34:25,725 - INFO - Processing split_07 - TEST\n",
      "2025-12-23 00:34:25,726 - INFO - ============================================================\n",
      "2025-12-23 00:34:25,777 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:34:25,810 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:34:30,747 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:34:30,941 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:34:31,441 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:34:32,266 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:34:32,642 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:34:33,201 - INFO - Combining all features...\n",
      "2025-12-23 00:34:33,203 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:34:33,205 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:34:33,206 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:34:33,208 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:34:33,209 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:34:33,211 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:34:33,213 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:34:33,215 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:34:33,217 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:34:33,218 - INFO - ✓ Completed split_07: 398 objects, 248 features\n",
      "2025-12-23 00:34:33,221 - INFO - ============================================================\n",
      "2025-12-23 00:34:33,221 - INFO - Processing split_08 - TRAIN\n",
      "2025-12-23 00:34:33,222 - INFO - ============================================================\n",
      "2025-12-23 00:34:33,246 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:34:33,261 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:34:35,057 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:34:35,132 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:34:35,331 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:34:35,673 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:34:35,825 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:34:36,068 - INFO - Combining all features...\n",
      "2025-12-23 00:34:36,070 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:34:36,071 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:34:36,073 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:34:36,074 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:34:36,076 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:34:36,077 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:34:36,080 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:34:36,081 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:34:36,083 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:34:36,085 - INFO - ✓ Completed split_08: 162 objects, 248 features\n",
      "2025-12-23 00:34:36,086 - INFO - ============================================================\n",
      "2025-12-23 00:34:36,086 - INFO - Processing split_08 - TEST\n",
      "2025-12-23 00:34:36,087 - INFO - ============================================================\n",
      "2025-12-23 00:34:36,138 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:34:36,172 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:34:41,052 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:34:41,309 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:34:41,913 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:34:42,730 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:34:43,075 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:34:43,795 - INFO - Combining all features...\n",
      "2025-12-23 00:34:43,797 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:34:43,799 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:34:43,801 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:34:43,803 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:34:43,805 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:34:43,808 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:34:43,811 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:34:43,814 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:34:43,818 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:34:43,819 - INFO - ✓ Completed split_08: 387 objects, 248 features\n",
      "2025-12-23 00:34:43,824 - INFO - ============================================================\n",
      "2025-12-23 00:34:43,824 - INFO - Processing split_09 - TRAIN\n",
      "2025-12-23 00:34:43,825 - INFO - ============================================================\n",
      "2025-12-23 00:34:43,853 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:34:43,870 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:34:45,453 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:34:45,552 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:34:45,770 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:34:46,073 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:34:46,210 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:34:46,435 - INFO - Combining all features...\n",
      "2025-12-23 00:34:46,437 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:34:46,439 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:34:46,441 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:34:46,442 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:34:46,444 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:34:46,446 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:34:46,448 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:34:46,451 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:34:46,453 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:34:46,454 - INFO - ✓ Completed split_09: 128 objects, 248 features\n",
      "2025-12-23 00:34:46,455 - INFO - ============================================================\n",
      "2025-12-23 00:34:46,456 - INFO - Processing split_09 - TEST\n",
      "2025-12-23 00:34:46,456 - INFO - ============================================================\n",
      "2025-12-23 00:34:46,502 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:34:46,534 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:34:50,004 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:34:50,172 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:34:50,586 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:34:51,245 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:34:51,522 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:34:52,058 - INFO - Combining all features...\n",
      "2025-12-23 00:34:52,060 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:34:52,062 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:34:52,063 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:34:52,066 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:34:52,068 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:34:52,070 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:34:52,072 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:34:52,075 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:34:52,077 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:34:52,078 - INFO - ✓ Completed split_09: 289 objects, 248 features\n",
      "2025-12-23 00:34:52,080 - INFO - ============================================================\n",
      "2025-12-23 00:34:52,080 - INFO - Processing split_10 - TRAIN\n",
      "2025-12-23 00:34:52,081 - INFO - ============================================================\n",
      "2025-12-23 00:34:52,113 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:34:52,132 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:34:53,985 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:34:54,067 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:34:54,296 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:34:54,640 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:34:54,795 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:34:55,049 - INFO - Combining all features...\n",
      "2025-12-23 00:34:55,051 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:34:55,053 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:34:55,055 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:34:55,057 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:34:55,058 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:34:55,060 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:34:55,064 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:34:55,067 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:34:55,070 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:34:55,071 - INFO - ✓ Completed split_10: 144 objects, 248 features\n",
      "2025-12-23 00:34:55,072 - INFO - ============================================================\n",
      "2025-12-23 00:34:55,072 - INFO - Processing split_10 - TEST\n",
      "2025-12-23 00:34:55,073 - INFO - ============================================================\n",
      "2025-12-23 00:34:55,131 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:34:55,167 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:34:59,506 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:34:59,685 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:00,156 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:00,918 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:01,290 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:01,842 - INFO - Combining all features...\n",
      "2025-12-23 00:35:01,844 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:01,846 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:01,849 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:01,852 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:01,854 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:01,857 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:01,859 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:01,862 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:01,865 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:01,866 - INFO - ✓ Completed split_10: 331 objects, 248 features\n",
      "2025-12-23 00:35:01,868 - INFO - ============================================================\n",
      "2025-12-23 00:35:01,869 - INFO - Processing split_11 - TRAIN\n",
      "2025-12-23 00:35:01,871 - INFO - ============================================================\n",
      "2025-12-23 00:35:01,901 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:01,921 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:04,002 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:04,083 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:04,300 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:04,641 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:04,785 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:05,040 - INFO - Combining all features...\n",
      "2025-12-23 00:35:05,042 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:05,043 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:05,045 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:05,047 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:05,049 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:05,051 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:05,053 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:05,056 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:05,058 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:05,059 - INFO - ✓ Completed split_11: 146 objects, 248 features\n",
      "2025-12-23 00:35:05,060 - INFO - ============================================================\n",
      "2025-12-23 00:35:05,061 - INFO - Processing split_11 - TEST\n",
      "2025-12-23 00:35:05,061 - INFO - ============================================================\n",
      "2025-12-23 00:35:05,111 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:05,142 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:09,047 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:09,215 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:09,716 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:10,481 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:10,749 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:11,275 - INFO - Combining all features...\n",
      "2025-12-23 00:35:11,277 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:11,278 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:11,280 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:11,282 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:11,284 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:11,286 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:11,289 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:11,291 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:11,293 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:11,294 - INFO - ✓ Completed split_11: 325 objects, 248 features\n",
      "2025-12-23 00:35:11,295 - INFO - ============================================================\n",
      "2025-12-23 00:35:11,296 - INFO - Processing split_12 - TRAIN\n",
      "2025-12-23 00:35:11,297 - INFO - ============================================================\n",
      "2025-12-23 00:35:11,324 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:11,342 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:13,284 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:13,365 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:13,582 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:13,929 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:14,071 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:14,353 - INFO - Combining all features...\n",
      "2025-12-23 00:35:14,355 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:14,356 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:14,358 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:14,360 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:14,362 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:14,363 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:14,366 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:14,368 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:14,371 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:14,372 - INFO - ✓ Completed split_12: 155 objects, 248 features\n",
      "2025-12-23 00:35:14,373 - INFO - ============================================================\n",
      "2025-12-23 00:35:14,373 - INFO - Processing split_12 - TEST\n",
      "2025-12-23 00:35:14,374 - INFO - ============================================================\n",
      "2025-12-23 00:35:14,431 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:14,465 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:18,723 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:18,980 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:19,448 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:20,198 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:20,479 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:21,068 - INFO - Combining all features...\n",
      "2025-12-23 00:35:21,070 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:21,072 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:21,074 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:21,075 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:21,077 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:21,079 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:21,081 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:21,084 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:21,086 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:21,087 - INFO - ✓ Completed split_12: 353 objects, 248 features\n",
      "2025-12-23 00:35:21,089 - INFO - ============================================================\n",
      "2025-12-23 00:35:21,091 - INFO - Processing split_13 - TRAIN\n",
      "2025-12-23 00:35:21,091 - INFO - ============================================================\n",
      "2025-12-23 00:35:21,119 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:21,137 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:22,878 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:22,955 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:23,158 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:23,475 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:23,629 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:23,901 - INFO - Combining all features...\n",
      "2025-12-23 00:35:23,903 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:23,905 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:23,907 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:23,910 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:23,912 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:23,914 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:23,916 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:23,919 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:23,921 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:23,923 - INFO - ✓ Completed split_13: 143 objects, 248 features\n",
      "2025-12-23 00:35:23,924 - INFO - ============================================================\n",
      "2025-12-23 00:35:23,924 - INFO - Processing split_13 - TEST\n",
      "2025-12-23 00:35:23,925 - INFO - ============================================================\n",
      "2025-12-23 00:35:23,992 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:24,032 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:28,707 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:28,923 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:29,438 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:30,259 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:30,566 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:31,194 - INFO - Combining all features...\n",
      "2025-12-23 00:35:31,196 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:31,198 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:31,200 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:31,202 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:31,204 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:31,206 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:31,208 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:31,210 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:31,212 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:31,213 - INFO - ✓ Completed split_13: 379 objects, 248 features\n",
      "2025-12-23 00:35:31,216 - INFO - ============================================================\n",
      "2025-12-23 00:35:31,217 - INFO - Processing split_14 - TRAIN\n",
      "2025-12-23 00:35:31,217 - INFO - ============================================================\n",
      "2025-12-23 00:35:31,248 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:31,266 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:33,213 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:33,298 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:33,530 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:33,875 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:34,023 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:34,294 - INFO - Combining all features...\n",
      "2025-12-23 00:35:34,297 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:34,299 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:34,301 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:34,303 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:34,305 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:34,307 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:34,310 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:34,313 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:34,315 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:34,316 - INFO - ✓ Completed split_14: 154 objects, 248 features\n",
      "2025-12-23 00:35:34,317 - INFO - ============================================================\n",
      "2025-12-23 00:35:34,318 - INFO - Processing split_14 - TEST\n",
      "2025-12-23 00:35:34,318 - INFO - ============================================================\n",
      "2025-12-23 00:35:34,381 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:34,421 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:38,636 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:38,825 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:39,322 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:40,091 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:40,371 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:40,943 - INFO - Combining all features...\n",
      "2025-12-23 00:35:40,944 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:40,946 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:40,948 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:40,950 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:40,953 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:40,955 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:40,958 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:40,960 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:40,963 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:40,964 - INFO - ✓ Completed split_14: 351 objects, 248 features\n",
      "2025-12-23 00:35:40,965 - INFO - ============================================================\n",
      "2025-12-23 00:35:40,966 - INFO - Processing split_15 - TRAIN\n",
      "2025-12-23 00:35:40,966 - INFO - ============================================================\n",
      "2025-12-23 00:35:40,997 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:41,014 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:42,995 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:43,078 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:43,299 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:43,664 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:43,816 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:44,104 - INFO - Combining all features...\n",
      "2025-12-23 00:35:44,106 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:44,109 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:44,111 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:44,113 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:44,115 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:44,117 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:44,120 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:44,123 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:44,125 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:44,126 - INFO - ✓ Completed split_15: 158 objects, 248 features\n",
      "2025-12-23 00:35:44,127 - INFO - ============================================================\n",
      "2025-12-23 00:35:44,128 - INFO - Processing split_15 - TEST\n",
      "2025-12-23 00:35:44,128 - INFO - ============================================================\n",
      "2025-12-23 00:35:44,181 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:44,217 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:48,344 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:48,523 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:49,002 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:49,764 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:50,118 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:50,712 - INFO - Combining all features...\n",
      "2025-12-23 00:35:50,714 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:50,716 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:50,718 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:50,720 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:50,722 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:50,724 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:50,726 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:50,728 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:50,731 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:50,732 - INFO - ✓ Completed split_15: 342 objects, 248 features\n",
      "2025-12-23 00:35:50,735 - INFO - ============================================================\n",
      "2025-12-23 00:35:50,735 - INFO - Processing split_16 - TRAIN\n",
      "2025-12-23 00:35:50,736 - INFO - ============================================================\n",
      "2025-12-23 00:35:50,766 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:50,784 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:52,698 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:52,784 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:53,014 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:35:53,408 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:35:53,575 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:35:53,868 - INFO - Combining all features...\n",
      "2025-12-23 00:35:53,871 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:35:53,873 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:35:53,875 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:35:53,878 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:35:53,880 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:35:53,882 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:35:53,884 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:35:53,886 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:35:53,889 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:35:53,890 - INFO - ✓ Completed split_16: 155 objects, 248 features\n",
      "2025-12-23 00:35:53,892 - INFO - ============================================================\n",
      "2025-12-23 00:35:53,892 - INFO - Processing split_16 - TEST\n",
      "2025-12-23 00:35:53,893 - INFO - ============================================================\n",
      "2025-12-23 00:35:53,988 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:35:54,032 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:35:58,527 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:35:58,724 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:35:59,234 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:36:00,016 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:36:00,320 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:36:00,912 - INFO - Combining all features...\n",
      "2025-12-23 00:36:00,914 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:36:00,916 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:36:00,918 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:36:00,921 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:36:00,923 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:36:00,925 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:36:00,927 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:36:00,930 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:36:00,933 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:36:00,934 - INFO - ✓ Completed split_16: 354 objects, 248 features\n",
      "2025-12-23 00:36:00,935 - INFO - ============================================================\n",
      "2025-12-23 00:36:00,936 - INFO - Processing split_17 - TRAIN\n",
      "2025-12-23 00:36:00,937 - INFO - ============================================================\n",
      "2025-12-23 00:36:00,966 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:36:00,983 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:36:02,952 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:36:03,041 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:36:03,280 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:36:03,645 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:36:03,856 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:36:04,162 - INFO - Combining all features...\n",
      "2025-12-23 00:36:04,164 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:36:04,166 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:36:04,169 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:36:04,171 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:36:04,174 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:36:04,176 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:36:04,179 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:36:04,182 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:36:04,184 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:36:04,186 - INFO - ✓ Completed split_17: 153 objects, 248 features\n",
      "2025-12-23 00:36:04,187 - INFO - ============================================================\n",
      "2025-12-23 00:36:04,188 - INFO - Processing split_17 - TEST\n",
      "2025-12-23 00:36:04,188 - INFO - ============================================================\n",
      "2025-12-23 00:36:04,256 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:36:04,296 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:36:08,726 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:36:08,994 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:36:09,454 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:36:10,228 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:36:10,517 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:36:11,081 - INFO - Combining all features...\n",
      "2025-12-23 00:36:11,083 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:36:11,085 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:36:11,087 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:36:11,090 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:36:11,091 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:36:11,093 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:36:11,095 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:36:11,097 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:36:11,099 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:36:11,100 - INFO - ✓ Completed split_17: 351 objects, 248 features\n",
      "2025-12-23 00:36:11,103 - INFO - ============================================================\n",
      "2025-12-23 00:36:11,103 - INFO - Processing split_18 - TRAIN\n",
      "2025-12-23 00:36:11,104 - INFO - ============================================================\n",
      "2025-12-23 00:36:11,131 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:36:11,146 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:36:12,950 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:36:13,028 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:36:13,244 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:36:13,573 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:36:13,722 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:36:13,991 - INFO - Combining all features...\n",
      "2025-12-23 00:36:13,993 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:36:13,995 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:36:13,997 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:36:13,999 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:36:14,001 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:36:14,003 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:36:14,006 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:36:14,008 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:36:14,011 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:36:14,012 - INFO - ✓ Completed split_18: 152 objects, 248 features\n",
      "2025-12-23 00:36:14,013 - INFO - ============================================================\n",
      "2025-12-23 00:36:14,014 - INFO - Processing split_18 - TEST\n",
      "2025-12-23 00:36:14,014 - INFO - ============================================================\n",
      "2025-12-23 00:36:14,068 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:36:14,101 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:36:18,441 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:36:18,638 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:36:19,120 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:36:19,887 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:36:20,297 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:36:20,877 - INFO - Combining all features...\n",
      "2025-12-23 00:36:20,879 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:36:20,881 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:36:20,884 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:36:20,886 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:36:20,888 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:36:20,891 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:36:20,893 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:36:20,896 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:36:20,898 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:36:20,899 - INFO - ✓ Completed split_18: 345 objects, 248 features\n",
      "2025-12-23 00:36:20,901 - INFO - ============================================================\n",
      "2025-12-23 00:36:20,901 - INFO - Processing split_19 - TRAIN\n",
      "2025-12-23 00:36:20,901 - INFO - ============================================================\n",
      "2025-12-23 00:36:20,931 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:36:20,947 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:36:22,930 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:36:23,010 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:36:23,249 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:36:23,824 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:36:24,093 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:36:24,469 - INFO - Combining all features...\n",
      "2025-12-23 00:36:24,472 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:36:24,474 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:36:24,477 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:36:24,480 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:36:24,483 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:36:24,487 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:36:24,491 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:36:24,494 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:36:24,497 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:36:24,498 - INFO - ✓ Completed split_19: 147 objects, 248 features\n",
      "2025-12-23 00:36:24,500 - INFO - ============================================================\n",
      "2025-12-23 00:36:24,501 - INFO - Processing split_19 - TEST\n",
      "2025-12-23 00:36:24,502 - INFO - ============================================================\n",
      "2025-12-23 00:36:24,578 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:36:24,623 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:36:30,055 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:36:30,269 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:36:30,823 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:36:31,643 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:36:31,983 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:36:32,672 - INFO - Combining all features...\n",
      "2025-12-23 00:36:32,674 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:36:32,675 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:36:32,678 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:36:32,679 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:36:32,682 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:36:32,684 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:36:32,686 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:36:32,689 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:36:32,691 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:36:32,691 - INFO - ✓ Completed split_19: 375 objects, 248 features\n",
      "2025-12-23 00:36:32,693 - INFO - ============================================================\n",
      "2025-12-23 00:36:32,693 - INFO - Processing split_20 - TRAIN\n",
      "2025-12-23 00:36:32,695 - INFO - ============================================================\n",
      "2025-12-23 00:36:32,727 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:36:32,743 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:36:34,636 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:36:34,726 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:36:34,960 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:36:35,321 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:36:35,480 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:36:35,760 - INFO - Combining all features...\n",
      "2025-12-23 00:36:35,762 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:36:35,764 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:36:35,766 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:36:35,768 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:36:35,770 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:36:35,772 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:36:35,775 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:36:35,777 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:36:35,780 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:36:35,781 - INFO - ✓ Completed split_20: 153 objects, 248 features\n",
      "2025-12-23 00:36:35,782 - INFO - ============================================================\n",
      "2025-12-23 00:36:35,783 - INFO - Processing split_20 - TEST\n",
      "2025-12-23 00:36:35,783 - INFO - ============================================================\n",
      "2025-12-23 00:36:35,867 - INFO - Extracting Level 1: Metadata features...\n",
      "2025-12-23 00:36:35,910 - INFO - Extracting Level 2: Statistical features per filter...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:36:40,266 - INFO - Extracting Level 3: Time-domain features...\n",
      "2025-12-23 00:36:40,453 - INFO - Extracting Level 4: Rise/Decay + Power-law features (CRITICAL!)...\n",
      "2025-12-23 00:36:40,929 - INFO - Extracting Level 5: Variability indices...\n",
      "2025-12-23 00:36:41,704 - INFO - Extracting Level 6: Unified Color features (Stats + Evolution)...\n",
      "2025-12-23 00:36:42,004 - INFO - Extracting Level 7: Per-filter shape fitting...\n",
      "2025-12-23 00:36:42,589 - INFO - Combining all features...\n",
      "2025-12-23 00:36:42,591 - INFO -   Merged counts: 6 features\n",
      "2025-12-23 00:36:42,592 - INFO -   Merged stats_flux: 72 features\n",
      "2025-12-23 00:36:42,595 - INFO -   Merged stats_err: 72 features\n",
      "2025-12-23 00:36:42,597 - INFO -   Merged snr: 8 features\n",
      "2025-12-23 00:36:42,598 - INFO -   Merged time_overall: 14 features\n",
      "2025-12-23 00:36:42,600 - INFO -   Merged rise_decay: 7 features\n",
      "2025-12-23 00:36:42,602 - INFO -   Merged variability: 12 features\n",
      "2025-12-23 00:36:42,604 - INFO -   Merged colors: 36 features\n",
      "2025-12-23 00:36:42,607 - INFO -   Merged shape_per_filt: 3 features\n",
      "2025-12-23 00:36:42,608 - INFO - ✓ Completed split_20: 358 objects, 248 features\n",
      "2025-12-23 00:36:42,610 - INFO - Concatenating all splits...\n",
      "2025-12-23 00:36:42,638 - INFO -   Train: 3043 objects, 248 features\n",
      "2025-12-23 00:36:42,638 - INFO -   Test:  7135 objects, 248 features\n",
      "2025-12-23 00:36:42,639 - INFO - Applying imputation...\n",
      "2025-12-23 00:36:42,745 - INFO -   Filled 0 for 11 count/logic columns.\n",
      "2025-12-23 00:36:42,791 - INFO -   ✓ Added 66 missing indicators\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 00:36:42,808 - INFO -   ✓ Kept 81343 NaNs for LightGBM native handling\n",
      "2025-12-23 00:36:42,811 - INFO - Applying feature selection...\n",
      "2025-12-23 00:36:42,811 - INFO - Starting FIXED feature selection...\n",
      "2025-12-23 00:36:42,857 - INFO -   After variance filter: 299 features\n",
      "2025-12-23 00:36:42,858 - INFO -   Removing correlated features (>0.95)...\n",
      "2025-12-23 00:36:43,283 - INFO -     → 235 features (dropped 64)\n",
      "2025-12-23 00:36:43,284 - INFO -   Computing LightGBM importance with 5-fold CV...\n",
      "2025-12-23 00:36:46,768 - INFO -   Selected top 150 features\n",
      "2025-12-23 00:36:46,769 - INFO -   Top 5: ['r_skew_Flux', 'color_g_r_max', 'g_q10_Flux', 'g_skew_Flux', 'M_abs_u']\n",
      "2025-12-23 00:36:46,874 - WARNING -   ⚠ Test has 67 extra cols, dropping\n",
      "2025-12-23 00:36:48,248 - INFO - ================================================================================\n",
      "2025-12-23 00:36:48,249 - INFO - FEATURE ENGINEERING COMPLETE!\n",
      "2025-12-23 00:36:48,250 - INFO - ================================================================================\n",
      "2025-12-23 00:36:48,250 - INFO - ✓ Train features: features_out_data_feature_selection\\train_features_all_splits.csv\n",
      "2025-12-23 00:36:48,251 - INFO -   - 3043 objects\n",
      "2025-12-23 00:36:48,252 - INFO -   - 154 total columns\n",
      "2025-12-23 00:36:48,253 - INFO -   - 150 feature columns\n",
      "2025-12-23 00:36:48,253 - INFO - ✓ Test features: features_out_data_feature_selection\\test_features_all_splits.csv\n",
      "2025-12-23 00:36:48,254 - INFO -   - 7135 objects\n",
      "2025-12-23 00:36:48,255 - INFO -   - 153 total columns\n",
      "2025-12-23 00:36:48,258 - INFO - \n",
      "Target distribution:\n",
      "2025-12-23 00:36:48,260 - INFO -   Non-TDE (0): 2895\n",
      "2025-12-23 00:36:48,261 - INFO -   TDE (1):     148\n",
      "2025-12-23 00:36:48,261 - INFO -   Ratio:       1:19.6\n",
      "2025-12-23 00:36:48,262 - INFO - ================================================================================\n",
      "2025-12-23 00:36:48,373 - INFO - \n",
      "================================================================================\n",
      "2025-12-23 00:36:48,374 - INFO - SUCCESS! Ready for modeling.\n",
      "2025-12-23 00:36:48,375 - INFO - ================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ENTRY POINT\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = Config()\n",
    "    # ====== CONFIGURATION - OPTIMIZED FOR TDE CLASSIFICATION ======\n",
    "          \n",
    "    cfg.do_imputation = True             \n",
    "\n",
    "    cfg.do_shape_fitting = True          \n",
    "    cfg.shape_fit_per_filter = True      \n",
    "    cfg.shape_fit_min_points = 6         \n",
    "    cfg.shape_fit_min_peak_snr = 5.0     \n",
    "    \n",
    "    # Enable feature selection\n",
    "    cfg.do_feature_selection = True   \n",
    "    cfg.rf_top_k = 150                  \n",
    "    \n",
    "    cfg.use_multiprocessing = False    \n",
    "    cfg.n_jobs = 4\n",
    "    \n",
    "    # ==============================================================\n",
    "\n",
    "    logger.info(\"Starting feature engineering with OPTIMIZED configuration...\")\n",
    "    logger.info(\"Key improvements:\")\n",
    "    logger.info(\"  ✓ Power-law vs exponential decay discrimination\")\n",
    "    logger.info(\"  ✓ UV excess (u-g color)\")\n",
    "    logger.info(\"  ✓ Color evolution rates\")\n",
    "    logger.info(\"  ✓ Per-filter shape fitting\")\n",
    "    logger.info(\"  ✓ Enhanced statistical features (skew, kurt, MAD)\")\n",
    "    logger.info(\"  ✓ Variability indices (Stetson J, Von Neumann)\")\n",
    "    logger.info(\"  ✓ Improved missing data handling\")\n",
    "    logger.info(\"  ✓ CV-based feature selection\")\n",
    "    \n",
    "    train_df, test_df = run_all(cfg)\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"SUCCESS! Ready for modeling.\")\n",
    "    logger.info(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
