# Cu·ªôc thi x√¢y d·ª±ng h·ªá th·ªëng Mallorn Astronomical Classification

<div align="center">

[![Kaggle](https://img.shields.io/badge/üèÜ_Kaggle_Competition-20BEFF?style=for-the-badge)](https://www.kaggle.com/competitions/mallorn-astronomical-classification-challenge/overview)
![Score](https://img.shields.io/badge/Public_Score-0.6004-success?style=for-the-badge)
[![AUC](https://img.shields.io/badge/OOF_AUC-0.9526-blue?style=for-the-badge)](#)

</div>
## 1. T·ªïng quan D·ª± √°n (Project Overview)

D·ª± √°n n√†y t·∫≠p trung gi·∫£i quy·∫øt b√†i to√°n ph√¢n lo·∫°i c√°c s·ª± ki·ªán **Tidal Disruption Events (TDE)** - hi·ªán t∆∞·ª£ng m·ªôt ng√¥i sao b·ªã h·ªë ƒëen si√™u kh·ªëi l∆∞·ª£ng x√© n√°t - t·ª´ d·ªØ li·ªáu kh·∫£o s√°t thi√™n vƒÉn ZTF.

ƒê√¢y l√† b√†i to√°n th√°ch th·ª©c v·ªõi d·ªØ li·ªáu chu·ªói th·ªùi gian th∆∞a th·ªõt (sparse lightcurves), t·ª∑ l·ªá m·∫•t c√¢n b·∫±ng nghi√™m tr·ªçng (**1:20**) v√† nhi·ªÖu n·ªÅn cao. Ph∆∞∆°ng ph√°p ti·∫øp c·∫≠n c·ªßa ch√∫ng t√¥i l√† **"Theory-Guided Data Science"** (Khoa h·ªçc d·ªØ li·ªáu d·∫´n ƒë∆∞·ªùng b·ªüi l√Ω thuy·∫øt), k·∫øt h·ª£p ki·∫øn th·ª©c v·∫≠t l√Ω thi√™n vƒÉn v√†o quy tr√¨nh Feature Engineering v√† t·ªëi ∆∞u h√≥a m√¥ h√¨nh b·∫±ng h·ªçc m√°y (Ensemble Learning).

---

## 2. C·∫•u tr√∫c Th∆∞ m·ª•c (Repository Structure)
```
‚îú‚îÄ‚îÄ cleaned_data/                     # D·ªØ li·ªáu sau khi ƒë√£ ti·ªÅn x·ª≠ l√Ω c∆° b·∫£n
‚îú‚îÄ‚îÄ features_out_data_feature_selection/ # Ch·ª©a file train/test features ƒë√£ qua ch·ªçn l·ªçc (Input cho Model)
‚îú‚îÄ‚îÄ eda.ipynb                         # Ph√¢n t√≠ch kh√°m ph√° d·ªØ li·ªáu (Exploratory Data Analysis)
‚îú‚îÄ‚îÄ preprocessing_data.ipynb          # B∆∞·ªõc 1: X·ª≠ l√Ω d·ªØ li·ªáu th√¥, kh·ª≠ Extinction, t√≠nh kho·∫£ng c√°ch
‚îú‚îÄ‚îÄ feature_engineering.ipynb         # B∆∞·ªõc 2: Tr√≠ch xu·∫•t 150+ ƒë·∫∑c tr∆∞ng v·∫≠t l√Ω (Main Script)
‚îú‚îÄ‚îÄ model.ipynb                       # B∆∞·ªõc 3 (V1): Hu·∫•n luy·ªán Baseline LightGBM v√† (V2): Th·ª≠ nghi·ªám v·ªõi m√¥ h√¨nh CatBoost
‚îú‚îÄ‚îÄ model_v2.ipynb                    # B∆∞·ªõc 3 (V3 Final): T·ªëi ∆∞u h√≥a Ensemble (LGBM + XGB + Cat)
‚îú‚îÄ‚îÄ ve_tde_non_tde.ipynb              # Tr·ª±c quan h√≥a so s√°nh Lightcurve TDE vs Non-TDE
‚îî‚îÄ‚îÄ README.md                         # T√†i li·ªáu d·ª± √°n
```

---

## 3. Lu·ªìng x·ª≠ l√Ω (Pipeline Architecture)

H·ªá th·ªëng ƒë∆∞·ª£c v·∫≠n h√†nh theo quy tr√¨nh kh√©p k√≠n t·ª´ x·ª≠ l√Ω d·ªØ li·ªáu th√¥ ƒë·∫øn d·ª± b√°o x√°c su·∫•t:
```mermaid
graph TB
    %% Define Styles
    classDef blue fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef purple fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef yellow fill:#fff9c4,stroke:#fbc02d,stroke-width:2px
    classDef green fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    
    %% Stage 1: PREPROCESSING
    subgraph P1["1. Preprocessing & Physics"]
        Raw[Raw ZTF Lightcurves]
        Extinct[De-extinction & Cosmology]
        Clean[Cleaned Flux & Abs Mag]
        Raw --> Extinct
        Extinct --> Clean
    end
    
    %% Stage 2: FEATURE ENGINEERING
    subgraph P2["2. Feature Extraction"]
        L2[Shape: Power-law vs Exp]
        L3[Thermo: Color u-g, Temp]
        L4[Stats: Skew, MAD, Stetson]
    end
    
    %% Stage 3: SELECTION
    subgraph P3["3. Feature Selection"]
        Filter[Variance & Correlation Filter]
        RFE[LightGBM RFE Selection]
        FinalSet[Top 150 Features]
        Filter --> RFE
        RFE --> FinalSet
    end
    
    %% Stage 4: MODELING
    subgraph P4["4. Weighted Ensemble"]
        LGBM[LightGBM - Optuna]
        XGB[XGBoost - Hist]
        CAT[CatBoost - Balanced]
        Opt[Nelder-Mead Optimization]
        Result[Final Probability]
        LGBM --> Opt
        XGB --> Opt
        CAT --> Opt
        Opt --> Result
    end
    
    %% Connections between stages
    Clean --> L2
    Clean --> L3
    Clean --> L4
    L2 --> Filter
    L3 --> Filter
    L4 --> Filter
    FinalSet --> LGBM
    FinalSet --> XGB
    FinalSet --> CAT
    
    %% Apply styles
    class P1 blue
    class P2 purple
    class P3 yellow
    class P4 green
```
---

## 4. ƒê·∫∑c tr∆∞ng V·∫≠t l√Ω (Theory-Guided Features)

H·ªá th·ªëng kh√¥ng s·ª≠ d·ª•ng th·ªëng k√™ m√π qu√°ng m√† tr√≠ch xu·∫•t c√°c ƒë·∫∑c tr∆∞ng d·ª±a tr√™n b·∫£n ch·∫•t v·∫≠t l√Ω c·ªßa TDE:

- **NƒÉng l∆∞·ª£ng (Energetics):** `M_abs_u` (ƒê·ªô s√°ng tuy·ªát ƒë·ªëi c·ª±c t√≠m) - Nh·∫≠n di·ªán s·ª± ki·ªán si√™u s√°ng
- **H√¨nh th√°i h·ªçc (Morphology):** `r_skew_Flux`, `rise_rate` - Ph√¢n bi·ªát d·∫°ng xung "TƒÉng nhanh - Gi·∫£m ch·∫≠m" c·ªßa TDE so v·ªõi h√¨nh sin c·ªßa sao bi·∫øn quang
- **Nhi·ªát ƒë·ªông h·ªçc (Thermodynamics):** `color_g_r_max` - TDE duy tr√¨ nhi·ªát ƒë·ªô v·∫≠t th·ªÉ ƒëen cao (m√†u xanh) ·ªïn ƒë·ªãnh, trong khi Supernova ngu·ªôi d·∫ßn (chuy·ªÉn ƒë·ªè)

---

## 5. Qu√° tr√¨nh M√¥ h√¨nh h√≥a & K·∫øt qu·∫£

Nh√≥m ƒë√£ tr·∫£i qua 3 giai ƒëo·∫°n t·ªëi ∆∞u h√≥a ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ cu·ªëi c√πng:

| Phi√™n b·∫£n | Chi·∫øn l∆∞·ª£c | Public Score | OOF AUC | Ghi ch√∫ k·ªπ thu·∫≠t |
|-----------|------------|--------------|---------|------------------|
| **V1** | Single LightGBM | 0.5684 | 0.8578 | Baseline t·ªët nh∆∞ng Variance cao, Recall th·∫•p (58%) |
| **V2** | CatBoost | 0.5652 | - | Th·∫•t b·∫°i do trung b√¨nh c·ªông ƒë∆°n gi·∫£n l√†m gi·∫£m hi·ªáu su·∫•t |
| **V3** | **Optimized Ensemble LGB + CatBoost + XGBoost** | **0.6004** | **0.9526** | **Th√†nh c√¥ng:** T·ªëi ∆∞u h√≥a tr·ªçng s·ªë b·∫±ng thu·∫≠t to√°n Nelder-Mead |

### C·∫•u h√¨nh Ensemble V3 (Final)

M√¥ h√¨nh cu·ªëi c√πng l√† s·ª± k·∫øt h·ª£p c√≥ tr·ªçng s·ªë c·ªßa 3 thu·∫≠t to√°n m·∫°nh nh·∫•t:

- **CatBoost (53%):** Ch·ªß ƒë·∫°o. C·∫•u h√¨nh `Auto-Balanced` v√† c√¢y ƒë·ªëi x·ª©ng gi√∫p x·ª≠ l√Ω nhi·ªÖu v∆∞·ª£t tr·ªôi
- **XGBoost (26%):** H·ªó tr·ª£. S·ª≠ d·ª•ng `tree_method='hist'` ƒë·ªÉ ·ªïn ƒë·ªãnh h√≥a d·ª± b√°o
- **LightGBM (21%):** T·∫•n c√¥ng. ƒê∆∞·ª£c tinh ch·ªânh Hyperparameter s√¢u b·∫±ng **Optuna**

---

## 6. H∆∞·ªõng d·∫´n C√†i ƒë·∫∑t & Ch·∫°y

### Y√™u c·∫ßu h·ªá th·ªëng
```bash
Python 3.8+
```

### Th∆∞ vi·ªán c·∫ßn thi·∫øt
```bash
pip install lightgbm catboost xgboost optuna scipy pandas numpy scikit-learn matplotlib seaborn
```

### C√°c b∆∞·ªõc th·ª±c thi

1. **Ti·ªÅn x·ª≠ l√Ω:** Ch·∫°y `preprocessing_data.ipynb` ƒë·ªÉ l√†m s·∫°ch d·ªØ li·ªáu th√¥
2. **T·∫°o ƒë·∫∑c tr∆∞ng:** Ch·∫°y `feature_engineering.ipynb`. ƒê√¢y l√† b∆∞·ªõc t·ªën th·ªùi gian nh·∫•t ƒë·ªÉ t·∫°o ra 150 features
3. **Hu·∫•n luy·ªán & D·ª± b√°o:** Ch·∫°y `model_v2.ipynb`. Script n√†y s·∫Ω:
   - T·ª± ƒë·ªông ch·∫°y Cross-Validation
   - T·ªëi ∆∞u h√≥a tham s·ªë
   - T√¨m tr·ªçng s·ªë Ensemble t·ªëi ∆∞u
   - Xu·∫•t file `submission.csv`

---

## 7. K·∫øt qu·∫£ ch√≠nh

- ‚úÖ **Public Score:** 0.6004
- ‚úÖ **OOF AUC:** 0.9526
- ‚úÖ **Ph∆∞∆°ng ph√°p:** Theory-Guided Feature Engineering + Optimized Ensemble
- ‚úÖ **ƒêi·ªÉm n·ªïi b·∫≠t:** K·∫øt h·ª£p ki·∫øn th·ª©c v·∫≠t l√Ω thi√™n vƒÉn v·ªõi machine learning ti√™n ti·∫øn

## 8. B√°o c√°o chi ti·∫øt t·∫°i file B√°o_C√°o_B√†i_T·∫≠p_L·ªõn.pdf
